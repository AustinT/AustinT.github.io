<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Austin Tripp's website</title><link>https://austintripp.ca/</link><description>Austin Tripp's personal website</description><atom:link href="https://austintripp.ca/rss.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2024 &lt;a href="mailto:austin.james.tripp[at]gmail.com"&gt;Austin Tripp&lt;/a&gt; MIT License</copyright><lastBuildDate>Mon, 07 Oct 2024 11:18:59 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Problems with the top-k diversity metric for diverse optimization</title><link>https://austintripp.ca/blog/2024-10-07-problems-with-topk-diversity/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered" id="cell-id=b4485680-e872-4498-b53b-cfc0bcde2315"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;div class="alert alert-block alert-info"&gt;
&lt;b&gt;NOTE&lt;/b&gt;
    this blog post can be run as a jupyter notebook. I re-ordered the cells to make it easier to read; to re-produce all the plots see instructions at the end of the post.
&lt;/div&gt;
&lt;h3 id="Background"&gt;Background&lt;a class="anchor-link" href="https://austintripp.ca/blog/2024-10-07-problems-with-topk-diversity/#Background"&gt;¶&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;em&gt;"Diverse optimization"&lt;/em&gt; has been a popular topic in machine learning conferences
for a few years now, particularly in the "AI for drug discovery" sub-field.
In this context, the goal of "optimization" algorithms is to suggest
promising drug candidates,
where "promising" means maximizing one (or more) objective functions.
An example of an objective function could be a docking score
(an approximate simulation of the interactions between a protein and a molecule).
"Diverse" optimization further requires that an algorithm produce multiple
distinct candidate solutions.
This is typically desired when the objective functions don't fully capture
everything we want (for example, a drug candidate also having low toxicity).
The hope is that a diverse set of candidates will have a higher chance of
one useful candidate compared to a non-diverse sets.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://austintripp.ca/blog/2024-10-07-problems-with-topk-diversity/"&gt;Read more…&lt;/a&gt; (7 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>machine learning</category><category>statistics</category><guid>https://austintripp.ca/blog/2024-10-07-problems-with-topk-diversity/</guid><pubDate>Sun, 06 Oct 2024 23:00:00 GMT</pubDate></item><item><title>How I chose a static site generator</title><link>https://austintripp.ca/blog/2024-09-09-static-site-generators/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;div&gt;&lt;p&gt;Recently, I wanted to update my website to look a bit more polished (and
support additional features such as automatically generating pages for my
publications). In the end I decided to completely switch from building my
website with Jekyll to &lt;a class="reference external" href="https://getnikola.com/"&gt;nikola&lt;/a&gt; instead. This post
explains my thought process for this (in case anybody else is considering
a similar switch).&lt;/p&gt;
&lt;p&gt;&lt;a href="https://austintripp.ca/blog/2024-09-09-static-site-generators/"&gt;Read more…&lt;/a&gt; (2 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>website</category><guid>https://austintripp.ca/blog/2024-09-09-static-site-generators/</guid><pubDate>Sun, 08 Sep 2024 23:00:00 GMT</pubDate></item><item><title>Reasons to have a website</title><link>https://austintripp.ca/blog/2024-08-26-reasons-website/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;div&gt;&lt;p&gt;I created this website because I thought (and continue to think) that having
a website can benefit one's career. Essentially, a professional website serves
as an accessible source of information about oneself for prospective employers,
coworkers, and employees. Unless you put something horrible on your website,
the effect should at worst be neutral, so there is essentially no downside to
having one.&lt;/p&gt;
&lt;p&gt;In the remainder of the post I will lay out a more detailed case for having
a website and address some potential hesitations people might have about
creating one.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://austintripp.ca/blog/2024-08-26-reasons-website/"&gt;Read more…&lt;/a&gt; (7 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>website</category><guid>https://austintripp.ca/blog/2024-08-26-reasons-website/</guid><pubDate>Sun, 25 Aug 2024 23:00:00 GMT</pubDate></item><item><title>The Monty Hall Problem is Really About Policy Assumptions</title><link>https://austintripp.ca/blog/2020/02/10/monty-hall/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;div&gt;&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;: the Monty Hall problem doesn't have a well-defined solution.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://austintripp.ca/blog/2020/02/10/monty-hall/"&gt;Read more…&lt;/a&gt; (8 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>frontpage</category><category>statistics</category><guid>https://austintripp.ca/blog/2020/02/10/monty-hall/</guid><pubDate>Mon, 10 Feb 2020 00:00:00 GMT</pubDate></item><item><title>A Quick Tutorial on Bash Quotes</title><link>https://austintripp.ca/blog/2019/07/18/bash-quotes/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;div&gt;&lt;p&gt;Today I learned &lt;strong&gt;way&lt;/strong&gt; more about quotations in bash than I ever thought I needed to know.
I thought I would highlight the interesting use case that I discovered, which requires some special trickery to write a script that executes arbitrary commands.
First, let's quickly review some facts about bash quotes.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://austintripp.ca/blog/2019/07/18/bash-quotes/"&gt;Read more…&lt;/a&gt; (8 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>frontpage</category><category>programming</category><guid>https://austintripp.ca/blog/2019/07/18/bash-quotes/</guid><pubDate>Wed, 17 Jul 2019 23:00:00 GMT</pubDate></item><item><title>How to Keep a Communal Fridge Clean</title><link>https://austintripp.ca/blog/2018/10/08/communal-fridge/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;div&gt;&lt;p&gt;Last month, my class decided that we should get a fridge for the class study room.
This brought up an important question: how would the fridge be cleaned?
I thought this was an interesting problem and deserved some discussion, both from a practical and a theoretical standpoint.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://austintripp.ca/blog/2018/10/08/communal-fridge/"&gt;Read more…&lt;/a&gt; (3 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><guid>https://austintripp.ca/blog/2018/10/08/communal-fridge/</guid><pubDate>Sun, 07 Oct 2018 23:00:00 GMT</pubDate></item><item><title>Language Travel Logs: Japanese 2018</title><link>https://austintripp.ca/blog/2018/09/19/japanese-learning-reflections/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;div&gt;&lt;p&gt;One dream I have always had since I started learning languages is to be able to go to another country and use that language to communicate.
This August I had the first opportunity to do that during a 2 week trip to Japan. 
In this post, I will outline the preparation I did before going, where I was able to use it when I was there, and evaluate my success.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://austintripp.ca/blog/2018/09/19/japanese-learning-reflections/"&gt;Read more…&lt;/a&gt; (6 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>japanese</category><category>language</category><guid>https://austintripp.ca/blog/2018/09/19/japanese-learning-reflections/</guid><pubDate>Tue, 18 Sep 2018 23:00:00 GMT</pubDate></item><item><title>Sparse Matrices: 5 Tips and Tricks</title><link>https://austintripp.ca/blog/2018/09/12/sparse-matrices-tips1/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;div&gt;&lt;p&gt;Over the course of my internship at the online shopping company &lt;a href="https://www.wish.com/"&gt;Wish&lt;/a&gt;, I have dealt a lot with a lot of data in the form of sparse matrices, specificaly in the form of item interaction matrices for customer data. In doing so, I have made heavy use of &lt;a href="https://docs.scipy.org/doc/scipy/reference/sparse.html"&gt;scipy's sparse matrices library&lt;/a&gt;.
Here are 5 tricks that I have learned.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://austintripp.ca/blog/2018/09/12/sparse-matrices-tips1/"&gt;Read more…&lt;/a&gt; (2 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>frontpage</category><category>machine learning</category><guid>https://austintripp.ca/blog/2018/09/12/sparse-matrices-tips1/</guid><pubDate>Tue, 11 Sep 2018 23:00:00 GMT</pubDate></item><item><title>Why you should never be certain of your beliefs: a Bayesian perspective</title><link>https://austintripp.ca/blog/2018/08/21/bayes-no-certain-priors/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;div&gt;&lt;p&gt;People are notoriously bad at estimating their percent confidence in their beliefs, as explained further in this &lt;a href="https://en.wikipedia.org/wiki/Overconfidence_effect"&gt;Wikipedia article&lt;/a&gt;. 
Something I thought of recently is what effect this overconfidence has from a Bayesian perspective. 
After a bit of math, I came to the conclusion that having extreme confidence in your beliefs (0% or 100% confidence) implies that you would be unable to change your beliefs if shown evidence to the contrary.
I believe this simple argument suggests that it is very irrational to hold prior beliefs of 0 or 100%. If you do feel this way, then you should choose a very high value (99.99%) or a very low value (0.001%), but always leave some room for error.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://austintripp.ca/blog/2018/08/21/bayes-no-certain-priors/"&gt;Read more…&lt;/a&gt; (4 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>decision making</category><category>frontpage</category><category>statistics</category><guid>https://austintripp.ca/blog/2018/08/21/bayes-no-certain-priors/</guid><pubDate>Mon, 20 Aug 2018 23:00:00 GMT</pubDate></item><item><title>An Overview of Gradient Boosting and Popular Libraries for it.</title><link>https://austintripp.ca/blog/2018/07/17/gradient-boosting/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;div&gt;&lt;p&gt;Everybody doing machine learning wants the best models possible. 
The aim of this blog article is the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;To provide an introduction to the machine learning technique known as &lt;em&gt;boosting&lt;/em&gt;, and specifically &lt;em&gt;gradient boosting&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;To compare/contrast boosting with other ensemble methods, such as &lt;em&gt;bagging&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;To explain and compare several popular gradient boosting frameworks, specifically &lt;em&gt;XGBoost&lt;/em&gt;, &lt;em&gt;CatBoost&lt;/em&gt;, and &lt;em&gt;LightGBM&lt;/em&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a href="https://austintripp.ca/blog/2018/07/17/gradient-boosting/"&gt;Read more…&lt;/a&gt; (7 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>frontpage</category><category>machine learning</category><guid>https://austintripp.ca/blog/2018/07/17/gradient-boosting/</guid><pubDate>Mon, 16 Jul 2018 23:00:00 GMT</pubDate></item></channel></rss>