<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Austin Tripp's website</title><link>https://austintripp.ca/</link><description>Austin Tripp's personal website</description><atom:link href="https://austintripp.ca/rss.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2025 &lt;a href="mailto:austin.james.tripp[at]gmail.com"&gt;Austin Tripp&lt;/a&gt; MIT License</copyright><lastBuildDate>Wed, 25 Jun 2025 08:01:59 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Blogging in the LLM age: a second golden age for blogs?</title><link>https://austintripp.ca/blog/2025-06-23-blog-in-llm-age/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;div&gt;&lt;p&gt;LLMs (large language models) are currently scraping &lt;em&gt;all&lt;/em&gt; text on the public
internet, training on it, and spitting out variants of that text in response to
queries. I think this fact makes now a &lt;em&gt;golden age&lt;/em&gt; for blog writing. If you
have ever thought about writing a blog, the time is now.&lt;/p&gt;
&lt;p&gt;This idea is not unique or original&lt;sup id="fnref:gwern"&gt;&lt;a class="footnote-ref" href="https://austintripp.ca/blog/2025-06-23-blog-in-llm-age/#fn:gwern"&gt;1&lt;/a&gt;&lt;/sup&gt;, but I am completely convinced by it. The
purpose of this post is to explain it in my own words.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://austintripp.ca/blog/2025-06-23-blog-in-llm-age/"&gt;Read more…&lt;/a&gt; (3 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>blog</category><category>speculation</category><guid>https://austintripp.ca/blog/2025-06-23-blog-in-llm-age/</guid><pubDate>Mon, 23 Jun 2025 00:00:00 GMT</pubDate></item><item><title>My review guide for machine learning conference papers.</title><link>https://austintripp.ca/blog/2025-06-22-ml-conference-review-guide/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;div&gt;&lt;p&gt;There is no official step by step guide for how to review ML conference papers
at venues like NeurIPS/ICML/ICLR.&lt;sup id="fnref:prev"&gt;&lt;a class="footnote-ref" href="https://austintripp.ca/blog/2025-06-22-ml-conference-review-guide/#fn:prev"&gt;1&lt;/a&gt;&lt;/sup&gt; In this post, I try to explain &lt;em&gt;my
guide&lt;/em&gt;. It is not official, endorsed, or necessarily good, but I have been
reviewing for 4+ years with this (implicitly) in mind already.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://austintripp.ca/blog/2025-06-22-ml-conference-review-guide/"&gt;Read more…&lt;/a&gt; (6 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>machine learning</category><category>peer review</category><guid>https://austintripp.ca/blog/2025-06-22-ml-conference-review-guide/</guid><pubDate>Sun, 22 Jun 2025 00:00:00 GMT</pubDate></item><item><title>Problems with ML for toxicity prediction.</title><link>https://austintripp.ca/blog/2025-05-13-ml-toxicity-pred/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;div&gt;&lt;p&gt;I recently read &lt;a href="https://pubs.acs.org/doi/10.1021/acs.chemrestox.5c00033"&gt;a review paper by Seal et
al&lt;/a&gt; about machine
learning for toxicity prediction. Given the length of the paper I thought I
would share my important takeaways. Disclaimer: not everything in this post was
part of the paper, and not everything in the paper is reflected in this post.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://austintripp.ca/blog/2025-05-13-ml-toxicity-pred/"&gt;Read more…&lt;/a&gt; (4 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>chemistry</category><category>machine learning</category><category>medicine</category><category>research papers</category><guid>https://austintripp.ca/blog/2025-05-13-ml-toxicity-pred/</guid><pubDate>Tue, 13 May 2025 00:00:00 GMT</pubDate></item><item><title>Chebyshev Scalarization Explained</title><link>https://austintripp.ca/blog/2025-05-12-chebyshev-scalarization/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;div&gt;&lt;p&gt;I've been reading about multi-objective optimization recently.&lt;sup id="fnref:surprise"&gt;&lt;a class="footnote-ref" href="https://austintripp.ca/blog/2025-05-12-chebyshev-scalarization/#fn:surprise"&gt;1&lt;/a&gt;&lt;/sup&gt;
Many papers state limitations of "linear scalarization" approaches,
mainly that it might not be able to represent all Pareto-optimal solutions
(if this sentence does not make sense to you, see &lt;a href="https://austintripp.ca/blog/2025-05-12-chebyshev-scalarization/#background-to-multi-objective-optimization"&gt;background&lt;/a&gt;).
Chebyshev scalarization is sometimes mentioned as an alternative which &lt;em&gt;can&lt;/em&gt; represent all solutions.
However, these papers mention it in passing without a proper explanation,
and I did not find a good explanation of it online.&lt;sup id="fnref:online"&gt;&lt;a class="footnote-ref" href="https://austintripp.ca/blog/2025-05-12-chebyshev-scalarization/#fn:online"&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;After doing a bit of my own research,&lt;sup id="fnref:gemini"&gt;&lt;a class="footnote-ref" href="https://austintripp.ca/blog/2025-05-12-chebyshev-scalarization/#fn:gemini"&gt;3&lt;/a&gt;&lt;/sup&gt; I found that Chebyshev scalarization is actually not too complicated, so I thought &lt;em&gt;I&lt;/em&gt; would explain it online.
In this post, I:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Give definitions for Chebyshev scalarization (for both maximization and minimization)&lt;/li&gt;
&lt;li&gt;Give a simple proof that it can represent all Pareto-optimal solutions&lt;/li&gt;
&lt;li&gt;Explain its relationship to linear scalarization via $\ell_p$ norms.&lt;/li&gt;
&lt;li&gt;Give a geometric interpretation via an interactive visualization&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="https://austintripp.ca/blog/2025-05-12-chebyshev-scalarization/"&gt;Read more…&lt;/a&gt; (13 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>bayesian optimization</category><category>machine learning</category><guid>https://austintripp.ca/blog/2025-05-12-chebyshev-scalarization/</guid><pubDate>Mon, 12 May 2025 00:00:00 GMT</pubDate></item><item><title>Reminder that Claude is really good for Chinese grammar</title><link>https://austintripp.ca/blog/2025-05-08-claude-chinese-grammar/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;div&gt;&lt;p&gt;This shouldn't surprise anybody who uses LLMs a lot, but LLMs are &lt;em&gt;really&lt;/em&gt; good
at common languages and translation. When practising Chinese today, a sentence
which confused me was:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://austintripp.ca/blog/2025-05-08-claude-chinese-grammar/"&gt;Read more…&lt;/a&gt; (1 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>chinese</category><category>language</category><category>using large language models</category><guid>https://austintripp.ca/blog/2025-05-08-claude-chinese-grammar/</guid><pubDate>Thu, 08 May 2025 00:00:00 GMT</pubDate></item><item><title>Some Beijing travel tips</title><link>https://austintripp.ca/blog/2025-04-28-beijing-travel/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;div&gt;&lt;p&gt;Earlier this year I went to Beijing. Here are some miscellaneous travel tips.
What I am writing here is probably not unique or original, treat this just as
my personal emphasis / endorsement.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://austintripp.ca/blog/2025-04-28-beijing-travel/"&gt;Read more…&lt;/a&gt; (1 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>travel</category><guid>https://austintripp.ca/blog/2025-04-28-beijing-travel/</guid><pubDate>Mon, 28 Apr 2025 00:00:00 GMT</pubDate></item><item><title>Taking the V out of VAEs: long live KL-regularized autoencoders!</title><link>https://austintripp.ca/blog/2025-04-24-v-out-of-vae/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;div&gt;&lt;p&gt;I recently read &lt;a href="https://sander.ai/2025/04/15/latents.html#"&gt;this post about generative modelling in latent
space&lt;/a&gt; by Sander Dieleman and
agreed strongly with the following quote (copied verbatim except for typo
correction):&lt;/p&gt;
&lt;p&gt;&lt;a href="https://austintripp.ca/blog/2025-04-24-v-out-of-vae/"&gt;Read more…&lt;/a&gt; (1 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>machine learning</category><guid>https://austintripp.ca/blog/2025-04-24-v-out-of-vae/</guid><pubDate>Thu, 24 Apr 2025 00:00:00 GMT</pubDate></item><item><title>Coding python packages with AI</title><link>https://austintripp.ca/blog/2025-04-12-ai-coding-python-package/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;div&gt;&lt;p&gt;I tried using some new LLM tools to code 2 entire python packages (instead of
editing a handful of lines at a time, which is what I did previously). It went
well! These tools are not perfect, but they are useful!&lt;/p&gt;
&lt;p&gt;&lt;a href="https://austintripp.ca/blog/2025-04-12-ai-coding-python-package/"&gt;Read more…&lt;/a&gt; (3 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>machine learning</category><category>using large language models</category><guid>https://austintripp.ca/blog/2025-04-12-ai-coding-python-package/</guid><pubDate>Sat, 12 Apr 2025 00:00:00 GMT</pubDate></item><item><title>Why your active learning algorithm may not do better than random</title><link>https://austintripp.ca/blog/2025-04-02-active-learning-random/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;div&gt;&lt;p&gt;I am a big fan of active learning, but I am also acutely aware of its potential
failure modes. A common failure mode is &lt;em&gt;random-like&lt;/em&gt; performance: achieving no
better "success"&lt;sup id="fnref:success"&gt;&lt;a class="footnote-ref" href="https://austintripp.ca/blog/2025-04-02-active-learning-random/#fn:success"&gt;1&lt;/a&gt;&lt;/sup&gt; in picking points than a &lt;em&gt;random&lt;/em&gt; policy. Indeed, it
is possible to experience this when the implementation is flawed.&lt;sup id="fnref:wrong"&gt;&lt;a class="footnote-ref" href="https://austintripp.ca/blog/2025-04-02-active-learning-random/#fn:wrong"&gt;2&lt;/a&gt;&lt;/sup&gt;
However, in some problems it &lt;em&gt;may not be possible&lt;/em&gt; to beat random-like
performance. In this post I try to explain why.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://austintripp.ca/blog/2025-04-02-active-learning-random/"&gt;Read more…&lt;/a&gt; (3 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>machine learning</category><category>speculation</category><guid>https://austintripp.ca/blog/2025-04-02-active-learning-random/</guid><pubDate>Wed, 02 Apr 2025 00:00:00 GMT</pubDate></item><item><title>Generic recommendations for cheminformatics models.</title><link>https://austintripp.ca/blog/2025-04-01-cheminformatics-model-qa/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;div&gt;&lt;p&gt;I was recently asked via email for my thoughts on which models to use &lt;em&gt;in
general&lt;/em&gt; for molecular property prediction. I'm sharing my responses publicly
in case they are useful.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://austintripp.ca/blog/2025-04-01-cheminformatics-model-qa/"&gt;Read more…&lt;/a&gt; (1 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>chemistry</category><category>machine learning</category><category>speculation</category><guid>https://austintripp.ca/blog/2025-04-01-cheminformatics-model-qa/</guid><pubDate>Tue, 01 Apr 2025 00:00:00 GMT</pubDate></item></channel></rss>