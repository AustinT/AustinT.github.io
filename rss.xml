<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Austin Tripp's website</title><link>https://austintripp.ca/</link><description>Austin Tripp's personal website</description><atom:link href="https://austintripp.ca/rss.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2025 &lt;a href="mailto:austin.james.tripp[at]gmail.com"&gt;Austin Tripp&lt;/a&gt; MIT License</copyright><lastBuildDate>Sun, 10 Aug 2025 15:45:56 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>An accessible introduction to mutual information using a d20</title><link>https://austintripp.ca/blog/2025-08-10-mutual-information-introduction/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered" id="cell-id=8c7d68eb-f957-4e21-8632-74b01147a07c"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;Scientists want to do "informative" experiments. What makes an experiment informative? I believe that information theory has a lot of answers. In this post I will try to explain information theory's definition of "how much information you get from an experiment" using d20 dice, assuming no previous knowledge of information theory.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://austintripp.ca/blog/2025-08-10-mutual-information-introduction/"&gt;Read more…&lt;/a&gt; (16 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>machine learning</category><category>statistics</category><guid>https://austintripp.ca/blog/2025-08-10-mutual-information-introduction/</guid><pubDate>Sun, 10 Aug 2025 00:00:00 GMT</pubDate></item><item><title>The case for public Slack channels only (no DMs).</title><link>https://austintripp.ca/blog/2025-07-27-public-slack-channels/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;div&gt;&lt;p&gt;Working at a company with a distributed team and hybrid work, a lot of work
gets done on Slack. Slack can be very noisy, and I used to wish that fewer
people would post in public channels, instead using direct messages (DMs).
However, I am starting to think that over the long run, defaulting to public
channels instead of DMs is best. Here, I try to explain why.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://austintripp.ca/blog/2025-07-27-public-slack-channels/"&gt;Read more…&lt;/a&gt; (2 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>opinion</category><guid>https://austintripp.ca/blog/2025-07-27-public-slack-channels/</guid><pubDate>Sun, 27 Jul 2025 00:00:00 GMT</pubDate></item><item><title>Tornados are just wind at close range.</title><link>https://austintripp.ca/blog/2025-07-26-tornados-are-wind/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;div&gt;&lt;p&gt;I find tornados really interesting: dark funnels of power and destruction. From
far away they look like a "wall of cloud", and over the last 6 months I spent a
bunch of time on YouTube trying to understand what a "wall of cloud" looks like
up close. The answer was a bit underwhelming:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://austintripp.ca/blog/2025-07-26-tornados-are-wind/"&gt;Read more…&lt;/a&gt; (5 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><guid>https://austintripp.ca/blog/2025-07-26-tornados-are-wind/</guid><pubDate>Sat, 26 Jul 2025 00:00:00 GMT</pubDate></item><item><title>Time zones: why you shouldn't use the terms EST or GMT</title><link>https://austintripp.ca/blog/2025-07-13-time-zones/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;div&gt;&lt;p&gt;This post is about a small mistake in how people discuss time zones which I
find mildly annoying: using the term EST (Eastern Standard Time) to refer to
the current time in cities like New York or Toronto, and GMT (Greenwich Mean
Time) to refer to the current time in the UK. Unfortunately these terms are
&lt;em&gt;not&lt;/em&gt; equivalent due to daylight saving time.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://austintripp.ca/blog/2025-07-13-time-zones/"&gt;Read more…&lt;/a&gt; (1 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>pedantry</category><guid>https://austintripp.ca/blog/2025-07-13-time-zones/</guid><pubDate>Sun, 13 Jul 2025 00:00:00 GMT</pubDate></item><item><title>The hardest bugs are non-bugs</title><link>https://austintripp.ca/blog/2025-07-12-hardest-bugs/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;div&gt;&lt;p&gt;Debugging computer programs is hard.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://austintripp.ca/blog/2025-07-12-hardest-bugs/"&gt;Read more…&lt;/a&gt; (1 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>programming</category><guid>https://austintripp.ca/blog/2025-07-12-hardest-bugs/</guid><pubDate>Sat, 12 Jul 2025 00:00:00 GMT</pubDate></item><item><title>Blogging in the LLM age: a second golden age for blogs?</title><link>https://austintripp.ca/blog/2025-06-23-blog-in-llm-age/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;div&gt;&lt;p&gt;LLMs (large language models) are currently scraping &lt;em&gt;all&lt;/em&gt; text on the public
internet, training on it, and spitting out variants of that text in response to
queries. I think this fact makes now a &lt;em&gt;golden age&lt;/em&gt; for blog writing. If you
have ever thought about writing a blog, the time is now.&lt;/p&gt;
&lt;p&gt;This idea is not unique or original&lt;sup id="fnref:gwern"&gt;&lt;a class="footnote-ref" href="https://austintripp.ca/blog/2025-06-23-blog-in-llm-age/#fn:gwern"&gt;1&lt;/a&gt;&lt;/sup&gt;, but I am completely convinced by it. The
purpose of this post is to explain it in my own words.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://austintripp.ca/blog/2025-06-23-blog-in-llm-age/"&gt;Read more…&lt;/a&gt; (3 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>blog</category><category>speculation</category><guid>https://austintripp.ca/blog/2025-06-23-blog-in-llm-age/</guid><pubDate>Mon, 23 Jun 2025 00:00:00 GMT</pubDate></item><item><title>My review guide for machine learning conference papers</title><link>https://austintripp.ca/blog/2025-06-22-ml-conference-review-guide/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;div&gt;&lt;p&gt;There is no official step by step guide for how to review ML conference papers
at venues like NeurIPS/ICML/ICLR.&lt;sup id="fnref:prev"&gt;&lt;a class="footnote-ref" href="https://austintripp.ca/blog/2025-06-22-ml-conference-review-guide/#fn:prev"&gt;1&lt;/a&gt;&lt;/sup&gt; In this post, I try to explain &lt;em&gt;my
guide&lt;/em&gt;. It is not official, endorsed, or necessarily good, but I have been
reviewing for 4+ years with this (implicitly) in mind already.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://austintripp.ca/blog/2025-06-22-ml-conference-review-guide/"&gt;Read more…&lt;/a&gt; (6 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>frontpage</category><category>machine learning</category><category>peer review</category><guid>https://austintripp.ca/blog/2025-06-22-ml-conference-review-guide/</guid><pubDate>Sun, 22 Jun 2025 00:00:00 GMT</pubDate></item><item><title>Problems with ML for toxicity prediction.</title><link>https://austintripp.ca/blog/2025-05-13-ml-toxicity-pred/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;div&gt;&lt;p&gt;I recently read &lt;a href="https://pubs.acs.org/doi/10.1021/acs.chemrestox.5c00033"&gt;a review paper by Seal et
al&lt;/a&gt; about machine
learning for toxicity prediction. Given the length of the paper I thought I
would share my important takeaways. Disclaimer: not everything in this post was
part of the paper, and not everything in the paper is reflected in this post.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://austintripp.ca/blog/2025-05-13-ml-toxicity-pred/"&gt;Read more…&lt;/a&gt; (4 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>chemistry</category><category>machine learning</category><category>medicine</category><category>research papers</category><guid>https://austintripp.ca/blog/2025-05-13-ml-toxicity-pred/</guid><pubDate>Tue, 13 May 2025 00:00:00 GMT</pubDate></item><item><title>Chebyshev Scalarization Explained</title><link>https://austintripp.ca/blog/2025-05-12-chebyshev-scalarization/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;div&gt;&lt;p&gt;I've been reading about multi-objective optimization recently.&lt;sup id="fnref:surprise"&gt;&lt;a class="footnote-ref" href="https://austintripp.ca/blog/2025-05-12-chebyshev-scalarization/#fn:surprise"&gt;1&lt;/a&gt;&lt;/sup&gt;
Many papers state limitations of "linear scalarization" approaches,
mainly that it might not be able to represent all Pareto-optimal solutions
(if this sentence does not make sense to you, see &lt;a href="https://austintripp.ca/blog/2025-05-12-chebyshev-scalarization/#background-to-multi-objective-optimization"&gt;background&lt;/a&gt;).
Chebyshev scalarization is sometimes mentioned as an alternative which &lt;em&gt;can&lt;/em&gt; represent all solutions.
However, these papers mention it in passing without a proper explanation,
and I did not find a good explanation of it online.&lt;sup id="fnref:online"&gt;&lt;a class="footnote-ref" href="https://austintripp.ca/blog/2025-05-12-chebyshev-scalarization/#fn:online"&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;After doing a bit of my own research,&lt;sup id="fnref:gemini"&gt;&lt;a class="footnote-ref" href="https://austintripp.ca/blog/2025-05-12-chebyshev-scalarization/#fn:gemini"&gt;3&lt;/a&gt;&lt;/sup&gt; I found that Chebyshev scalarization is actually not too complicated, so I thought &lt;em&gt;I&lt;/em&gt; would explain it online.
In this post, I:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Give definitions for Chebyshev scalarization (for both maximization and minimization)&lt;/li&gt;
&lt;li&gt;Give a simple proof that it can represent all Pareto-optimal solutions&lt;/li&gt;
&lt;li&gt;Explain its relationship to linear scalarization via $\ell_p$ norms.&lt;/li&gt;
&lt;li&gt;Give a geometric interpretation via an interactive visualization&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="https://austintripp.ca/blog/2025-05-12-chebyshev-scalarization/"&gt;Read more…&lt;/a&gt; (13 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>bayesian optimization</category><category>frontpage</category><category>machine learning</category><guid>https://austintripp.ca/blog/2025-05-12-chebyshev-scalarization/</guid><pubDate>Mon, 12 May 2025 00:00:00 GMT</pubDate></item><item><title>Reminder that Claude is really good for Chinese grammar</title><link>https://austintripp.ca/blog/2025-05-08-claude-chinese-grammar/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;div&gt;&lt;p&gt;This shouldn't surprise anybody who uses LLMs a lot, but LLMs are &lt;em&gt;really&lt;/em&gt; good
at common languages and translation. When practising Chinese today, a sentence
which confused me was:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://austintripp.ca/blog/2025-05-08-claude-chinese-grammar/"&gt;Read more…&lt;/a&gt; (1 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>chinese</category><category>language</category><category>using large language models</category><guid>https://austintripp.ca/blog/2025-05-08-claude-chinese-grammar/</guid><pubDate>Thu, 08 May 2025 00:00:00 GMT</pubDate></item></channel></rss>