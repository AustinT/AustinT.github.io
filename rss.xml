<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Austin Tripp's website</title><link>https://austintripp.ca/</link><description>Austin Tripp's personal website</description><atom:link href="https://austintripp.ca/rss.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2024 &lt;a href="mailto:austin.james.tripp[at]gmail.com"&gt;Austin Tripp&lt;/a&gt; MIT License</copyright><lastBuildDate>Sun, 18 Aug 2024 11:29:35 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>The Monty Hall Problem is Really About Policy Assumptions</title><link>https://austintripp.ca/blog/2020/02/10/monty-hall/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;: the Monty Hall problem doesn't have a well-defined solution.&lt;/p&gt;
&lt;p&gt;The Monty Hall problem is a classic "probability paradox"&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="https://austintripp.ca/blog/2020/02/10/monty-hall/#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt;, where the answer
is counter-intuitive to most people's intuitions. To paraphrase &lt;a href="https://en.wikipedia.org/wiki/Monty_Hall_problem"&gt;Wikipedia's
introduction to the
problem&lt;/a&gt;&lt;sup id="fnref:2"&gt;&lt;a class="footnote-ref" href="https://austintripp.ca/blog/2020/02/10/monty-hall/#fn:2"&gt;2&lt;/a&gt;&lt;/sup&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Suppose you're on a game show, and you're given the choice of three doors:
Behind one door is a prize; behind the others, nothing. You pick a door, say No. 1,
and the host, who knows what's behind the doors, opens another door, say No. 3,
which has nothing. He then says to you, "Do you want to pick door No. 2?" Is it
to your advantage to switch your choice? &lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This can be reframed as "what is the probability that the prize is behind door 2?"
The "&lt;em&gt;intuitive answer&lt;/em&gt;" is 1/2 (meaning there is no advantage to switching),
since initially the prize could be behind any of the 3 doors, and you find out
that it isn't behind door #3, leaving just 2 doors left with equal probability
of having the prize.&lt;/p&gt;
&lt;p&gt;In contrast, the "&lt;em&gt;correct answer&lt;/em&gt;" is 2/3. This discrepancy is usually
explained away as the host &lt;em&gt;giving away&lt;/em&gt; information with their choice of which
door to open. Again, paraphrasing
&lt;a href="https://en.wikipedia.org/wiki/Monty_Hall_problem#Simple_solutions"&gt;Wikipedia&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;An intuitive explanation is that, if the contestant initially picks an empty door (2
of 3 doors), the contestant will win the prize by switching because the other
empty door can no longer be picked, whereas if the contestant initially picks the prize
(1 of 3 doors), the contestant will not win the prize by switching.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;However, in this post I am arguing that &lt;strong&gt;neither of these answers are
"correct"&lt;/strong&gt;, because the answer ultimately depends on your assumptions on what
actions the game show host would take in counterfactual situations. Since this
is not specified in most versions of the problem, the problem can't really be
said to have a "&lt;em&gt;correct answer&lt;/em&gt;". After &lt;a href="https://austintripp.ca/blog/2020/02/10/monty-hall/#bayesian-formulation-of-the-problem"&gt;introducing the math necessary to solve the problem&lt;/a&gt;, I will work through some &lt;a href="https://austintripp.ca/blog/2020/02/10/monty-hall/#examples"&gt;example
assumptions&lt;/a&gt; and their resulting implications, and conclude with the
surprising result that under the right assumptions, the answer to the problem
could be &lt;a href="https://austintripp.ca/blog/2020/02/10/monty-hall/#rigged-game-the-answer-could-be-anything"&gt;&lt;strong&gt;any&lt;/strong&gt; probability&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Bayesian Formulation of the Problem&lt;/h3&gt;
&lt;p&gt;Let:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;\(c\) be the contestant's &lt;strong&gt;c&lt;/strong&gt;oice of door&lt;/li&gt;
&lt;li&gt;\(o\) be the door the host &lt;strong&gt;o&lt;/strong&gt;pens&lt;/li&gt;
&lt;li&gt;\(a\) be the &lt;strong&gt;a&lt;/strong&gt;nswer (the door the prize is actually behind)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In a Bayesian decision framework, the way to handle this problem is to:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Use your knowledge of the situation to construct a probability distribution over the relevant variables (\(p(o,a,c)\)).&lt;/li&gt;
&lt;li&gt;Use the values of the observed variables \(c\) and \(o\) to calculate the &lt;em&gt;posterior probability distribution&lt;/em&gt; over the answers: \(p(a\mid o,c)\).&lt;/li&gt;
&lt;li&gt;Use this probability distribution to make an informed choice. In this situation, reasonable choice would be to choose the most likely answer, \(a^* = \arg\max_a p(a\mid o,c)\).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Therefore solving the problem is the same as finding the posterior distribution.
The posterior distribution can be found using Bayes theorem:&lt;/p&gt;
&lt;p&gt;$$p(a\mid o,c) = \frac{p(o,a,c)}{p(o,c)} = \frac{p(o,a,c)}{\sum_{a'} p(o,a',c)}$$&lt;/p&gt;
&lt;p&gt;This is far as we can go without making any assumptions. The most basic
assumptions (which aren't that interesting) is that the game is fair (\(a\) is
chosen randomly in advance) and real (the player doesn't know anything about
\(a\), so their choice is effectively random).
This means that the only interesting behaviour is that of the host,
who can choose which door to open, knowing both where the prize is and the player's choice.
Mathematically, this means that the distribution \(p(o,a,c)\) can be &lt;em&gt;factored&lt;/em&gt; as:&lt;/p&gt;
&lt;p&gt;$$p(o,a,c) = p(a)p(c)p(o\mid a,c)$$&lt;/p&gt;
&lt;p&gt;Substituting this into the posterior equation above, and nothing that \(p(c)\) is
a constant and cancels, we find:&lt;/p&gt;
&lt;p&gt;$$p(a\mid o,c) = \frac{p(o,a,c)}{\sum_{a'} p(o,a',c)} = \frac{p(o\mid a,c)p(a)p(c)}{\sum_{a'}p(o\mid a',c)p(a')p(c)} = \frac{p(o\mid a,c)p(a)}{\sum_{a'}p(o\mid a',c)p(a')}$$&lt;/p&gt;
&lt;p&gt;Assuming further that all answers are equally probable, this further simplifies to:&lt;/p&gt;
&lt;p&gt;$$p(a\mid o,c) = \frac{p(o\mid a,c)}{\sum_{a'}p(o\mid a',c)}$$&lt;/p&gt;
&lt;p&gt;This means that for this particular problem,
the posterior can be computed with only 3 values:
\(p(o=3\mid a_i, c=1)\), \(i\in{1,2,3}\).&lt;/p&gt;
&lt;h3&gt;Examples&lt;/h3&gt;
&lt;h4&gt;Standard assumptions: the argument for 2/3&lt;/h4&gt;
&lt;p&gt;When most people hear this problem, beyond the assumptions outlined in
&lt;a href="https://austintripp.ca/blog/2020/02/10/monty-hall/#bayesian-formulation-of-the-problem"&gt;the previous section&lt;/a&gt;, they usually assume the following things:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;the host will not open the door with the prize behind it&lt;/li&gt;
&lt;li&gt;the host will not open the player's door&lt;/li&gt;
&lt;li&gt;the host will always open a door&lt;/li&gt;
&lt;li&gt;if the host can open multiple doors, they will choose one at random&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Assumptions 1-2 make sense, because otherwise the game show might not be any
fun to watch. Assumption 3 isn't necessary but is usually assumed because when
people imagine this game show they usually imagine that this switching gimmick
is just part of the show. Assumption 4 makes sense, since there is no reason &lt;em&gt;a
priori&lt;/em&gt; to assume that the host will be biased towards any particular door.&lt;/p&gt;
&lt;p&gt;With this, we can calculate the 3 values of \(p(o\mid a,c)\):&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;\(p(o=3\mid a=1,c=1)=1/2\): the host could open doors 2 or 3 here, and would
   therefore pick at random. This would have them pick door 3 50% of the time.&lt;/li&gt;
&lt;li&gt;\(p(o=3\mid a=2,c=1)=1\): here the host cannot choose door 1 (since the player
   chose it) or door #2 (since it has the prize), so opening door #3 is the only
   option&lt;/li&gt;
&lt;li&gt;\(p(o=3\mid a=3,c=1)=0\): if door #3 had the prize, the host wouldn't have opened
   it, so this event isn't possible.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;So, \(p(a=2\mid o=3,c=1) = \frac{1}{1/2+1} = 2/3\),
suggesting that the best choice is to pick door 2 (switch doors).
This is the classic "correct answer" of 2/3.&lt;/p&gt;
&lt;h4&gt;Biased random choice of host&lt;/h4&gt;
&lt;p&gt;Suppose that assumption 4 above isn't true:
when the host has multiple doors that they could choose,
they don't make a truly random decision&lt;sup id="fnref:3"&gt;&lt;a class="footnote-ref" href="https://austintripp.ca/blog/2020/02/10/monty-hall/#fn:3"&gt;3&lt;/a&gt;&lt;/sup&gt;.
One example of this is if the host tends to pick higher numbers over lower numbers:
that is, given a choice of opening doors 2 or 3, they will pick door 3 with probability \(z\),
which might not be \(1/2\).&lt;/p&gt;
&lt;p&gt;This scenario is similar to the standard one, except \(p(o=3\mid a=1,c=1)=z\), so
\(p(a=2\mid o=3,c=1)=\frac{1}{1+z}\), which can vary between 1/2 and 1 for different
values of \(z\). Already, this formulation (which isn't completely unrealistic)
gives a huge range of possible answers, all contingent on your belief about how
the host picks random numbers.&lt;/p&gt;
&lt;h4&gt;Inattentive host&lt;/h4&gt;
&lt;p&gt;Suppose the host didn't see what door the player chose. The host's decision
then can't depend on \(c\) (breaking assumption 2). Instead, the host will
randomly open one of the doors that doesn't have the prize, regardless of
whether the player chose this door. This means \(p(o=3\mid a=i,c=1)=1/2\) for
\(i=1,2\) (\(i=3\) remains unchanged since the host won't open the door with
the answer behind it). Therefore, \(p(a=2|o=3,c=1)=1/2\). This scenario is
certainly plausible in a real game show (although fairly unlikely).&lt;/p&gt;
&lt;h4&gt;No door as an option&lt;/h4&gt;
&lt;p&gt;If the host can choose &lt;em&gt;not&lt;/em&gt; to open a door, this opens up a wide range of possibilities:&lt;/p&gt;
&lt;h5&gt;Dramatic host 1&lt;/h5&gt;
&lt;p&gt;To make the show more entertaining to watch, the host wants to bait the player
into switching doors, not getting the prize, and then watch them agonize over
their decision to switch. One way to do this would be to only open a door if
the player's initial guess was correct.&lt;/p&gt;
&lt;p&gt;This means \(p(o=3\mid a=1,c=1)=1/2\), and the other 2 probabilities are 0, so the
posterior probability \(p(a=2\mid o=3,c=1)=0\).&lt;/p&gt;
&lt;h5&gt;Dramatic host 2&lt;/h5&gt;
&lt;p&gt;Dramatic host 1 is too predictable (since being offered to switch doors means
you should never switch). Dramatic host 2 becomes slightly less predictable by
choosing with probability \(q\) to open a wrong door if the player initially
chose correctly. This means that \(p(o=3\mid a=1,c=1)=1/2\) (like last time), but
\(p(o=3\mid a=2,c=1)=q\), making the posterior probability
\(p(a=2\mid o=3,c=1)=\frac{q}{1/2+q}\), which can range from 0 to 2/3.&lt;/p&gt;
&lt;h5&gt;Merciful host&lt;/h5&gt;
&lt;p&gt;Instead, a host could open a door only if the player's initial choice was
wrong, yielding a posterior probability \(p(a=2\mid o=3,c=1)=1\). In this case, you
would always want to switch doors if offered.&lt;/p&gt;
&lt;h5&gt;Only door 3 can be opened&lt;/h5&gt;
&lt;p&gt;Imagine that the host will act normally (i.e. standard assumptions), but if the
final decision is to open doors 1 or 2 they will simply not open a door at all
(they are only willing to open door 3)&lt;sup id="fnref:4"&gt;&lt;a class="footnote-ref" href="https://austintripp.ca/blog/2020/02/10/monty-hall/#fn:4"&gt;4&lt;/a&gt;&lt;/sup&gt;. This means that \(p(o=3\mid a=1,c=1)=1\),
so the posterior \(p(a=2\mid o=3,c=1)=1/2\).&lt;/p&gt;
&lt;h3&gt;Rigged game: the answer could be anything&lt;/h3&gt;
&lt;p&gt;Suppose that our initial model of the causal process is wrong, and the game is
unfair. Specifically, the player chooses a door, and then the correct door is
chosen afterwards with this information&lt;sup id="fnref:5"&gt;&lt;a class="footnote-ref" href="https://austintripp.ca/blog/2020/02/10/monty-hall/#fn:5"&gt;5&lt;/a&gt;&lt;/sup&gt;. 
Specifically, suppose that the producers select the player's door as correct
with probability \(q\), and one of the other 2 doors with probability \(1-q\).
Note that if \(q=1/3\), this corresponds to the standard assumptions of a fair game.&lt;/p&gt;
&lt;p&gt;To calculate the posterior in this scenario, we need \(p(a\mid c)\) and \(p(o\mid a,c)\),
which are the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;\(p(a=1\mid c=1) = q\) (by assumption above)&lt;/li&gt;
&lt;li&gt;\(p(a=2\mid c=1) = p(a=3\mid c=1) = (1-q)/2\) (since the probabilities must add to 1)&lt;/li&gt;
&lt;li&gt;Since the hosts actions don't change, \(p(o\mid a,c)\) is the same as before
  (\(p(o=3\mid a=1,c=1)=1/2\), \(p(o=3\mid a=2,c=1)=1\), \(p(o=3\mid a=3,c=1)=0\))&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Reapplying Bayes rule, we get:&lt;/p&gt;
&lt;p&gt;$$p(a\mid o,c) = \frac{p(o\mid a,c)p(a\mid c)}{\sum_{a'}p(o\mid a',c)p(a'\mid c)}$$&lt;/p&gt;
&lt;p&gt;$$p(a=2\mid o=3,c=1) = \frac{1\times(1-q)/2}{q/2 + 1\times(1-q)/2 + 0\times(1-q)/2} = 1-q$$&lt;/p&gt;
&lt;p&gt;Since \(q\) can take on any value between 0 and 1, the posterior value can be any
value between 0 and 1, meaning that the right decision could be &lt;strong&gt;absolutely
anything&lt;/strong&gt;. Of course, this is far from what most people assume when they
encounter the Monty Hall problem, but if you were actually on such a game show
in real life and offered this deal, would it not be prudent to at least
consider this possibility?&lt;/p&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;In this post, I've shown how &lt;a href="https://austintripp.ca/blog/2020/02/10/monty-hall/#bayesian-formulation-of-the-problem"&gt;Bayes rule&lt;/a&gt;
can be used to solve the Monty Hall problem, which under &lt;a href="https://austintripp.ca/blog/2020/02/10/monty-hall/#standard-assumptions-the-argument-for-23"&gt;standard
assumptions&lt;/a&gt; gives the "correct
answer" of 2/3. However, under different assumptions, the posterior probability
of the prize being behind door 2 can take on many different values, which
between a &lt;a href="https://austintripp.ca/blog/2020/02/10/monty-hall/#biased-random-choice-of-host"&gt;biased host&lt;/a&gt; and a host that wants to
&lt;a href="https://austintripp.ca/blog/2020/02/10/monty-hall/#dramatic-host-2"&gt;make you lose&lt;/a&gt; spans the whole range of probabilities
between 0 and 1. In fact, just assuming that the game isn't fair yields a model
where it could be better to switch with probabilities ranging from 0 to
1 inclusive!.&lt;/p&gt;
&lt;p&gt;Overall, it's very misleading to suggest that this is just a simple problem
that tests if you can do some algebra; the entire solution is &lt;strong&gt;completely
determined&lt;/strong&gt; by one's beliefs about the host's counterfactual actions, so it's
frankly surprising that this aspect of the problem isn't more widely discussed.
As always, while an explicit Bayesian formulation isn't required to arrive at
the desired answer, the Bayesian framework does aid greatly in making your
assumptions explicit.&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;Specifically it would be a &lt;a href="https://en.wikipedia.org/wiki/Paradox#Quine's_classification"&gt;veridical paradox&lt;/a&gt;, because the result appears absurd but is nonetheless true. &lt;a class="footnote-backref" href="https://austintripp.ca/blog/2020/02/10/monty-hall/#fnref:1" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:2"&gt;
&lt;p&gt;the original version used cars and goats, which I changed to just a prize/nothing for clarity. &lt;a class="footnote-backref" href="https://austintripp.ca/blog/2020/02/10/monty-hall/#fnref:2" title="Jump back to footnote 2 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:3"&gt;
&lt;p&gt;since people are generally quite bad at choosing truly random numbers &lt;a class="footnote-backref" href="https://austintripp.ca/blog/2020/02/10/monty-hall/#fnref:3" title="Jump back to footnote 3 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:4"&gt;
&lt;p&gt;maybe they are superstitious and like the number 3? &lt;a class="footnote-backref" href="https://austintripp.ca/blog/2020/02/10/monty-hall/#fnref:4" title="Jump back to footnote 4 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:5"&gt;
&lt;p&gt;The producers of the game show might do this to increase the amount of dramatic tension on the show. Who says that "reality" TV needs to be real? &lt;a class="footnote-backref" href="https://austintripp.ca/blog/2020/02/10/monty-hall/#fnref:5" title="Jump back to footnote 5 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description><guid>https://austintripp.ca/blog/2020/02/10/monty-hall/</guid><pubDate>Mon, 10 Feb 2020 00:00:00 GMT</pubDate></item><item><title>A Quick Tutorial on Bash Quotes</title><link>https://austintripp.ca/blog/2019/07/18/bash-quotes/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;p&gt;Today I learned &lt;strong&gt;way&lt;/strong&gt; more about quotations in bash than I ever thought I needed to know.
I thought I would highlight the interesting use case that I discovered, which requires some special trickery to write a script that executes arbitrary commands.
First, let's quickly review some facts about bash quotes.&lt;/p&gt;
&lt;h3&gt;Quick review of bash quotes (skip this if you're an expert)&lt;/h3&gt;
&lt;p&gt;When you type a command into bash, it evaluates any special characters or sequences before executing the command.
As a basic example, consider the following commands&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="https://austintripp.ca/blog/2019/07/18/bash-quotes/#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt;:&lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;$&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;echo&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;$HOME&lt;/span&gt;
/home/austin
$&lt;span class="w"&gt; &lt;/span&gt;python&lt;span class="w"&gt; &lt;/span&gt;-c&lt;span class="w"&gt; &lt;/span&gt;print&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;7&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
bash:&lt;span class="w"&gt; &lt;/span&gt;syntax&lt;span class="w"&gt; &lt;/span&gt;error&lt;span class="w"&gt; &lt;/span&gt;near&lt;span class="w"&gt; &lt;/span&gt;unexpected&lt;span class="w"&gt; &lt;/span&gt;token&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="sb"&gt;`&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="err"&gt;'&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The first fragment uses the command &lt;code&gt;echo&lt;/code&gt;, which naturally echos the argument to feed to it.
The argument fed to it is $HOME, which due to the dollar sign is interpreted as a bash variable.
Therefore it evaluates the contents of this variable, which is my home directory (/home/austin).&lt;/p&gt;
&lt;p&gt;The second example runs python, using the &lt;code&gt;-c&lt;/code&gt; command to execute python code passed as a string.
&lt;code&gt;print(7)&lt;/code&gt; is a completely valid python command that prints the number 7,
however parentheses are special characters in bash.
So, before that string makes it to the &lt;code&gt;python&lt;/code&gt; command,
bash attempts to evaluate it, determines that it is invalid bash syntax (since it's not bash it's python), and throws an error.
Obviously that's not what that program is supposed to do.&lt;/p&gt;
&lt;p&gt;Hence the purpose of quotes: they are used to prevent bash from evaluating text that you would like to leave as a string.&lt;/p&gt;
&lt;h4&gt;Single quotes&lt;/h4&gt;
&lt;p&gt;The most basic kind of quote is the single quote (\').
This quote completely prevents bash from evaluating a command.
Let's look at a few examples.&lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;$&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;echo&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s1"&gt;'$HOME'&lt;/span&gt;
&lt;span class="nv"&gt;$HOME&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This command literally prints the string "$HOME", since nothing was evaluated.
Now is also maybe a good time to point out that the first $ character on the first line is the shell prompt (the last character on the line where you type in commands), so it isn't actually part of the command.
Another example:&lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;$&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s1"&gt;'echo $HOME'&lt;/span&gt;
&lt;span class="nb"&gt;echo&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;$HOME&lt;/span&gt;:&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;command&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;not&lt;span class="w"&gt; &lt;/span&gt;found
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Putting quotes around the whole command prevents bash from parsing it at all, not even breaking up the input into a command and an argument.
So it just assumes that the literal string "echo $HOME" is a command, and doesn't know what command that is, so it gives an error.
One final variant would be:&lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;$&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s1"&gt;'echo'&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;$HOME&lt;/span&gt;
/home/austin
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This command does the same thing as the initial command:
it doesn't evaluate echo (which bash wouldn't modify anyways),
then evaluates &lt;code&gt;$HOME&lt;/code&gt; since that wasn't in quotes.&lt;/p&gt;
&lt;p&gt;Moving onto the second example, we can make the python code run directly by enclosing the program in quotes.&lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;$&lt;span class="w"&gt; &lt;/span&gt;python&lt;span class="w"&gt; &lt;/span&gt;-c&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s1"&gt;'print(7)'&lt;/span&gt;
&lt;span class="m"&gt;7&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;h4&gt;Double Quotes&lt;/h4&gt;
&lt;p&gt;The double quote (\") is also a valid quote in bash, but it works a bit differently.
Think of it as a &lt;em&gt;selective quote&lt;/em&gt;: it lets bash evaluate some things but not others.
As explained in more detail in the 
&lt;a href="https://www.gnu.org/software/bash/manual/html_node/Double-Quotes.html#Double-Quotes"&gt;bash manual&lt;/a&gt;,
this quote stops all evaluations except for $, \, and `.
For the examples above:&lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;$&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;echo&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt;&lt;span class="nv"&gt;$HOME&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt;
/home/austin
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;As stated, since $HOME has a \$ the quotes don't stop bash from evaluating it.&lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;$&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"echo &lt;/span&gt;&lt;span class="nv"&gt;$HOME&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt;
bash:&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;echo&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;/home/austin:&lt;span class="w"&gt; &lt;/span&gt;No&lt;span class="w"&gt; &lt;/span&gt;such&lt;span class="w"&gt; &lt;/span&gt;file&lt;span class="w"&gt; &lt;/span&gt;or&lt;span class="w"&gt; &lt;/span&gt;directory
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This one is funny: bash evaluates &lt;code&gt;$HOME&lt;/code&gt; since it has a \$, but then doesn't evaluate the expression as a whole and treats it as a command.
Although for some reason it treats it as a file instead of a command, and says it can't find the file.&lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;$&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"echo"&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;$HOME&lt;/span&gt;
/home/austin
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Of course, this one does the same thing as the single quotes case.&lt;/p&gt;
&lt;p&gt;With the python example, single and double quotes make no different:&lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;$&lt;span class="w"&gt; &lt;/span&gt;python&lt;span class="w"&gt; &lt;/span&gt;-c&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"print(7)"&lt;/span&gt;
&lt;span class="m"&gt;7&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This is because the double quotes prevent the evaluation of the parentheses.&lt;/p&gt;
&lt;h4&gt;Quick recap&lt;/h4&gt;
&lt;p&gt;So essentially:
- Without quotes, bash tries to evaluate all special characters
- Single quotes (\') prevent all evaluation
- Double quotes (\") prevent most evaluation, but notably not the evaluation of variables&lt;/p&gt;
&lt;h3&gt;Nested Commands: the case of eval&lt;/h3&gt;
&lt;p&gt;Where things get weird is when you need to nest commands.
A particularly nasty command is &lt;code&gt;eval&lt;/code&gt;, which evaluates arbitrary bash commands.
In the examples below, it doesn't really make sense why you would need to use &lt;code&gt;eval&lt;/code&gt;, but ignore that for now: a real example will come at the end.&lt;/p&gt;
&lt;p&gt;Let's go back to our first example:&lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;$&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;eval&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;echo&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;$HOME&lt;/span&gt;
/home/austin
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Pretty much what you would expect: the variable &lt;code&gt;$HOME&lt;/code&gt; gets replaced.
Now what if we wanted to use &lt;code&gt;eval&lt;/code&gt; to print the literal string &lt;code&gt;$HOME&lt;/code&gt;?
From above, we can put &lt;code&gt;$HOME&lt;/code&gt; in single quotes:&lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;$&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;eval&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;echo&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s1"&gt;'$HOME'&lt;/span&gt;
/home/austin
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;However, this doesn't work. Why? Because nested commands effectively get evaluated twice.
Just look at the following example:&lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;$&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;echo&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;echo&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s1"&gt;'$HOME'&lt;/span&gt;
&lt;span class="nb"&gt;echo&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;$HOME&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The evaluation procedure effectively has 2 stages:
1. Run through the string and do substitutions. In this case, due to the single quotes the part &lt;code&gt;$HOME&lt;/code&gt; is just treated as the literal string "\$HOME".
2. Execute the command, which here is to echo the string &lt;code&gt;echo $HOME&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;So in the case of &lt;code&gt;eval&lt;/code&gt;, it bash first parses the command, performing no substitution for &lt;code&gt;$HOME&lt;/code&gt;, and then it evaluates the resulting command, since that's what &lt;code&gt;eval&lt;/code&gt; does.
In this case, the command to be evaluated is the string &lt;code&gt;echo $HOME&lt;/code&gt;, which as shown in the very first example above results in &lt;code&gt;$HOME&lt;/code&gt; being substituted.&lt;/p&gt;
&lt;p&gt;So, working backwards, the solution is to somehow pass the string "&lt;code&gt;echo '$HOME'&lt;/code&gt;" to &lt;code&gt;eval&lt;/code&gt;.
A natural approach to do this would be to quote the entire phrase you don't want bash to evaluate, so we will place the whole phrase &lt;code&gt;echo '$HOME'&lt;/code&gt; in single quotes.&lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;$&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;eval&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s1"&gt;'echo '&lt;/span&gt;&lt;span class="nv"&gt;$HOME&lt;/span&gt;&lt;span class="s1"&gt;''&lt;/span&gt;
/home/austin
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;It didn't work! What happened?
This one took me a long time to figure out, but the answer is that bash parses the string from left to right, not inside to outside like a nested function call in most programming language.
Let's try it without the &lt;code&gt;eval&lt;/code&gt;:&lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;$&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s1"&gt;'echo '&lt;/span&gt;&lt;span class="nv"&gt;$HOME&lt;/span&gt;&lt;span class="s1"&gt;''&lt;/span&gt;
bash:&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;echo&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;/home/austin:&lt;span class="w"&gt; &lt;/span&gt;No&lt;span class="w"&gt; &lt;/span&gt;such&lt;span class="w"&gt; &lt;/span&gt;file&lt;span class="w"&gt; &lt;/span&gt;or&lt;span class="w"&gt; &lt;/span&gt;directory
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This is the same result as the command &lt;code&gt;"echo $HOME"&lt;/code&gt; from earlier!
Evaluating literally left to right, it actually sees it as the string "&lt;code&gt;echo&lt;/code&gt;" 
(interpreted literally so including the space),
the variable &lt;code&gt;$HOME&lt;/code&gt;, which has no quotes beside it so it is evaluated to /home/austin,
and finally an empty string between 2 quotes.
It then concatenates the two strings "&lt;code&gt;echo&lt;/code&gt;" and "&lt;code&gt;/home/austin&lt;/code&gt;" and treats it as a single string,
therefore giving the same result as using double quotes from earlier.
So the double quoting simply can't be done this way.&lt;/p&gt;
&lt;p&gt;In the end, I found two solutions to this problem: showing them both here.&lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;$&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;eval&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;echo&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="se"&gt;\'&lt;/span&gt;&lt;span class="s1"&gt;'$HOME'&lt;/span&gt;&lt;span class="se"&gt;\'&lt;/span&gt;
&lt;span class="nv"&gt;$HOME&lt;/span&gt;
$&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;eval&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;echo&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"'"&lt;/span&gt;&lt;span class="s1"&gt;'$HOME'&lt;/span&gt;&lt;span class="s2"&gt;"'"&lt;/span&gt;
&lt;span class="nv"&gt;$HOME&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Both solutions take advantage of the fact that when two strings are beside each other, they are concatenated.
A quick example:&lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;$&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;echo&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"hel""lo"&lt;/span&gt;
hello
$&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;echo&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;hel&lt;span class="s2"&gt;"lo"&lt;/span&gt;
hello
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Here, the two strings "hel" and "lo" are simply concatenated when executing the command.
The second example clarifies that both strings do not have to be in quotes.
The two &lt;code&gt;eval&lt;/code&gt; commands above work in much the same way.
1. The string &lt;code&gt;echo&lt;/code&gt; is concatenated with the escaped literal &lt;code&gt;\'&lt;/code&gt;, which is taken to simply be a single quote string due to the backslash. This is concatenated with &lt;code&gt;'$HOME'&lt;/code&gt;, which just gets evaluated to the string &lt;code&gt;$HOME&lt;/code&gt;, and finally another literal single quote is appended.
 The result of the evaluation is that &lt;code&gt;eval&lt;/code&gt; is passed the literal string &lt;code&gt;echo '$HOME'&lt;/code&gt; as desired.
2. Much like the first, except that instead of escaping the single quotes they are enclosed in double quotes, since they block the evaluation of the single quotes
(remember the only exceptions to double quotes are $, \, and `, not \'). So the end result is the same.&lt;/p&gt;
&lt;p&gt;Both of these solutions look pretty gross, but to put it in perspective, imagine what you have to do to use a doubly nested &lt;code&gt;eval&lt;/code&gt;!
To save you the trouble of imagining, you have to do this:&lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;$&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;eval&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;eval&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;echo&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="se"&gt;\\\'\'&lt;/span&gt;&lt;span class="s1"&gt;'$HOME'&lt;/span&gt;&lt;span class="se"&gt;\'\\\'&lt;/span&gt;
&lt;span class="nv"&gt;$HOME&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Figuring it out is left as an exercise to the reader (try replacing the first &lt;code&gt;eval&lt;/code&gt; with &lt;code&gt;echo&lt;/code&gt; to see how all the escaped characters are evaluated).&lt;/p&gt;
&lt;h3&gt;The script&lt;/h3&gt;
&lt;p&gt;So, what exactly was the use case that made me encounter a weird problem like this?
Essentially, at the time of writing this post I was working at a research institute with a computing cluster than ran the job manager &lt;a href="https://slurm.schedmd.com/overview.html"&gt;slurm&lt;/a&gt;.
The basic workflow of slurm is that you write a script that runs your job, then you submit the script to a job manager.
Naively, you need one bash script for each program you run, but I was running a lot of programs with similar setup, so I wanted to just write one script.
Eventually I came up with something like this:&lt;/p&gt;
&lt;div class="code"&gt;&lt;table class="codetable"&gt;&lt;tr&gt;&lt;td class="linenos linenodiv"&gt;&lt;a href="https://austintripp.ca/blog/2019/07/18/bash-quotes/#-1"&gt;&lt;code data-line-number="1"&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;code&gt;&lt;span class="ch"&gt;#!/bin/bash&lt;/span&gt;
&lt;/code&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="linenos linenodiv"&gt;&lt;a href="https://austintripp.ca/blog/2019/07/18/bash-quotes/#-2"&gt;&lt;code data-line-number="2"&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;code&gt;&lt;span class="c1"&gt;#&lt;/span&gt;
&lt;/code&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="linenos linenodiv"&gt;&lt;a href="https://austintripp.ca/blog/2019/07/18/bash-quotes/#-3"&gt;&lt;code data-line-number="3"&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;code&gt;&lt;span class="c1"&gt;#SBATCH --job-name=test&lt;/span&gt;
&lt;/code&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="linenos linenodiv"&gt;&lt;a href="https://austintripp.ca/blog/2019/07/18/bash-quotes/#-4"&gt;&lt;code data-line-number="4"&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;code&gt;&lt;span class="c1"&gt;#SBATCH --output=res.txt&lt;/span&gt;
&lt;/code&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="linenos linenodiv"&gt;&lt;a href="https://austintripp.ca/blog/2019/07/18/bash-quotes/#-5"&gt;&lt;code data-line-number="5"&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;code&gt;&lt;span class="c1"&gt;#&lt;/span&gt;
&lt;/code&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="linenos linenodiv"&gt;&lt;a href="https://austintripp.ca/blog/2019/07/18/bash-quotes/#-6"&gt;&lt;code data-line-number="6"&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;code&gt;&lt;span class="c1"&gt;#SBATCH --ntasks=1&lt;/span&gt;
&lt;/code&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="linenos linenodiv"&gt;&lt;a href="https://austintripp.ca/blog/2019/07/18/bash-quotes/#-7"&gt;&lt;code data-line-number="7"&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;code&gt;&lt;span class="c1"&gt;#SBATCH --time=10:00&lt;/span&gt;
&lt;/code&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="linenos linenodiv"&gt;&lt;a href="https://austintripp.ca/blog/2019/07/18/bash-quotes/#-8"&gt;&lt;code data-line-number="8"&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;code&gt;
&lt;/code&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="linenos linenodiv"&gt;&lt;a href="https://austintripp.ca/blog/2019/07/18/bash-quotes/#-9"&gt;&lt;code data-line-number="9"&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;code&gt;&lt;span class="nb"&gt;eval&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt;&lt;span class="nv"&gt;$@&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt;
&lt;/code&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;
&lt;p&gt;This clever script just calls &lt;code&gt;eval&lt;/code&gt; on all the arguments passed to it (represented by the variable &lt;code&gt;$@&lt;/code&gt;).
The double quotes are essentially a safety measure (it is &lt;a href="http://tldp.org/LDP/abs/html/quotingvar.html"&gt;generally recommended&lt;/a&gt; to quote all variables in double quotes).
Another nice part of it is that it can be called with either &lt;code&gt;source&lt;/code&gt; to run it locally or &lt;code&gt;sbatch&lt;/code&gt; to submit it to the job scheduler.&lt;/p&gt;
&lt;p&gt;The script works great when you are doing something simple.
For example:&lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;$&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;source&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;script.sh&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;pwd&lt;/span&gt;
/home/austin
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This works because &lt;code&gt;pwd&lt;/code&gt; is a nice simple command: no weird characters.
What if I want to run a python script?
Let's try the following:&lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;$&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;source&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;script.sh&lt;span class="w"&gt; &lt;/span&gt;python&lt;span class="w"&gt; &lt;/span&gt;-c&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"print(7)"&lt;/span&gt;
bash:&lt;span class="w"&gt; &lt;/span&gt;eval:&lt;span class="w"&gt; &lt;/span&gt;line&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="m"&gt;9&lt;/span&gt;:&lt;span class="w"&gt; &lt;/span&gt;syntax&lt;span class="w"&gt; &lt;/span&gt;error&lt;span class="w"&gt; &lt;/span&gt;near&lt;span class="w"&gt; &lt;/span&gt;unexpected&lt;span class="w"&gt; &lt;/span&gt;token&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="sb"&gt;`&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;
&lt;span class="s1"&gt;bash: eval: line 9: `python -c print(7)'&lt;/span&gt;
$&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;source&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;script.sh&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s1"&gt;'python -c "print(7)"'&lt;/span&gt;
&lt;span class="m"&gt;7&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The first example fails because the quotes are "used up" on the initial evaluation, so there are no quotes when &lt;code&gt;eval&lt;/code&gt; is called in the script.
The second example avoids this by putting the whole thing in single quotes, thus delaying the evaluation until &lt;code&gt;eval&lt;/code&gt; is called in the script.&lt;/p&gt;
&lt;p&gt;But what if you wanted to print a more complicated string, like "Only $1? Let\'s go!".
This string has 3 challenging symbols: "\$", "\'", and "!".
Let's try to print it.
Of course unlike printing 7, we want to print a string, so we will need to put an additional set of quotes around the string.
Python allows strings to be used with single or double quotes,
but unfortunately using either will cause the number of single quotes to be imbalanced.
Furthermore, the "$1" and the "!" need to be escaped with single quotes or they will evaluate to something.
There are probably multiple solutions to this, but what I ended up doing was:&lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;$&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;source&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;script.sh&lt;span class="w"&gt; &lt;/span&gt;python&lt;span class="w"&gt; &lt;/span&gt;-c&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="se"&gt;\'&lt;/span&gt;&lt;span class="s1"&gt;'print('&lt;/span&gt;&lt;span class="se"&gt;\"&lt;/span&gt;Only&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s1"&gt;'$1'&lt;/span&gt;?&lt;span class="w"&gt; &lt;/span&gt;Let&lt;span class="se"&gt;\'\\\'\'&lt;/span&gt;s&lt;span class="w"&gt; &lt;/span&gt;go&lt;span class="se"&gt;\!\"&lt;/span&gt;&lt;span class="s1"&gt;')'&lt;/span&gt;&lt;span class="se"&gt;\'&lt;/span&gt;
Only&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;$1&lt;/span&gt;?&lt;span class="w"&gt; &lt;/span&gt;Let&lt;span class="err"&gt;'&lt;/span&gt;s&lt;span class="w"&gt; &lt;/span&gt;go!
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;To be honest I hate quotation marks now.&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;Evaluated in Ubuntu terminal running bash. &lt;a class="footnote-backref" href="https://austintripp.ca/blog/2019/07/18/bash-quotes/#fnref:1" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description><guid>https://austintripp.ca/blog/2019/07/18/bash-quotes/</guid><pubDate>Wed, 17 Jul 2019 23:00:00 GMT</pubDate></item><item><title>How to Keep a Communal Fridge Clean</title><link>https://austintripp.ca/blog/2018/10/08/communal-fridge/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;p&gt;Last month, my class decided that we should get a fridge for the class study room.
This brought up an important question: how would the fridge be cleaned?
I thought this was an interesting problem and deserved some discussion, both from a practical and a theoretical standpoint.&lt;/p&gt;
&lt;h3&gt;Description of problem&lt;/h3&gt;
&lt;p&gt;The problem with cleaning a fridge obviously isn't the actual cleaning process,
but rather construction of a system to assign cleaning duties to a variety of people.
It is a difficult problem for the following reasons:
- Since each person only utilizes a fraction of the fridge, they naturally don't feel responsible for cleaning the whole fridge.
 So it is unlikely that people will spontaneously clean it 
 (or at least people might be reluctant to).
- Some people use the fridge different amounts: one person might never put anything in it, and one person might store several weeks' worth of food.
 These people shouldn't have equal fridge-cleaning obligations.&lt;/p&gt;
&lt;p&gt;Because of this, we thought that a solution to the fridge cleaning problem must have the following properties:
1. Distribute the workload over a large number of people (i.e. don't assign a permanent fridge cleaner)
2. Only obligate people who use the fridge
3. Be decentralized (i.e. not rely on some sort of "leader" to assign the jobs and check to make sure it is done).
4. Be failure-resistant: if the job isn't done properly, there should be some sort of mechanism to make sure it is done properly eventually.&lt;/p&gt;
&lt;h3&gt;Volunteer Solution&lt;/h3&gt;
&lt;p&gt;The solution put forth by the group was to just put a volunteer list on the fridge, and people would sign up for a cleaning week.
This solution would work if people using the fridge were mature enough to volunteer for a week.
It is also distributed, self-running, and doesn't obligate people who don't use the fridge.
For this reason, it was very popular with the rest of the group.&lt;/p&gt;
&lt;h3&gt;Free Market Solution&lt;/h3&gt;
&lt;p&gt;I thought of a more interesting solution: why not create a market for cleaning, and use the principles of market economics to keep the fridge clean?
The problem with a volunteer-based solution is that people's only incentive to volunteer is a sense of decency and common good: a sense that not everybody has.
Instead, if motivated by money, then that gives people a tangible incentive to do work.&lt;/p&gt;
&lt;p&gt;For this solution, I envision a donation jar for a "fridge cleaning fund", that would be used to pay people to clean the fridge.
Each week, a "cleaning job" would be posted at a designated price, and somebody interested could do the job and take the reward.
The price would probably be just enough to buy something at the campus coffee/doughnut shop ($1-2), which means a cleaner could get a tangible reward
of their choice for their cleaning efforts.
If nobody volunteers to clean it (maybe because it is really messy), then the price of the cleaning job would be increased until somebody is willing to take it.
Thus, the free market is used to determine the difficulty of the cleaning job via its price, 
and the cleaner is fairly compensated for their work at the market rate.&lt;/p&gt;
&lt;p&gt;This novel solution would satisfy all the criteria because it distributes the workload over all those who are willing to do it,
doesn't obligate anybody (they can freely choose to accept the job), is decentralized (the market runs itself),
and resists failure because of its natural mechanism to raise the price of a difficult cleaning job.
The main downside however is that in the end, it depends on people's decency to donate to the fridge cleaning fund.
If not enough people donate, then the fund won't be able to pay people properly.
Moreover, the prices and price increases would have to be carefully planned: 
they must be high enough to motivate people to do work, but not so high as to drain the fund too fast.&lt;/p&gt;
&lt;h3&gt;Collective Punishment Solution&lt;/h3&gt;
&lt;p&gt;This solution is inspired by the military, where if a squad as a whole fails, they are often punished as a whole.
This solution would require a centralized leader, but their responsibility would be fairly minimal.
A requirement would be set out that the fridge has to be cleaned every week, and if nobody does it, the leader 
throws everything in the fridge in the garbage, and unplugs the fridge for a week (so nobody can use it).
People who want to use the fridge will not want to lose access to it and get their stuff thrown out, 
which will motivate somebody or a group of people to do the cleaning,
because the cost of not doing so is very high.
It essentially forces self-organization to avoid a very undesirable fate.
This solution is interesting from the organizer's perspective, because no organized list has to be created,
but unfortunately does require a central leader to coordinate it.&lt;/p&gt;
&lt;h3&gt;Conclusions&lt;/h3&gt;
&lt;p&gt;In the end, the class representatives chose the volunteer solution, because we think our class is fairly mature,
and it is conceptually the simplest system.
However, I do really like the free market solution, because of all the neat properties it has (like a self-adjusting price).
It felt like something straight out of an ECON 101 textbook.
Hopefully the volunteer system will stand the test of time.&lt;/p&gt;</description><guid>https://austintripp.ca/blog/2018/10/08/communal-fridge/</guid><pubDate>Sun, 07 Oct 2018 23:00:00 GMT</pubDate></item><item><title>Language Travel Logs: Japanese 2018</title><link>https://austintripp.ca/blog/2018/09/19/japanese-learning-reflections/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;p&gt;One dream I have always had since I started learning languages is to be able to go to another country and use that language to communicate.
This August I had the first opportunity to do that during a 2 week trip to Japan. 
In this post, I will outline the preparation I did before going, where I was able to use it when I was there, and evaluate my success.&lt;/p&gt;
&lt;h3&gt;Preparation&lt;/h3&gt;
&lt;p&gt;Before starting university in 2014, I knew absolutely no Japanese. 
In fact, the only foreign language I knew was a little bit of French.
Starting from absolutely nothing, my history of learning Japanese is the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;em&gt;2014-09 to 2016-08&lt;/em&gt;: learn a lot of Chinese, including simplified characters.
 Although Chinese is a different language than Japanese, both languages use Chinese characters, and there is a lot of shared vocabulary between the two languages.
 So, I count it as progress towards learning Japanese.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;2016-02 to 2016-04&lt;/em&gt;: used flashcards to learn Hiragana/Katakana, the "alphabet"
 of Japanese&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="https://austintripp.ca/blog/2018/09/19/japanese-learning-reflections/#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt;. Started to look through Tae Kim's guide to Japanese&lt;sup id="fnref:2"&gt;&lt;a class="footnote-ref" href="https://austintripp.ca/blog/2018/09/19/japanese-learning-reflections/#fn:2"&gt;2&lt;/a&gt;&lt;/sup&gt;, stopping 
 after learning the usage of the particles "wa" and "ga"&lt;/li&gt;
&lt;li&gt;2017-01 to 2017-04: Finish reading Tae Kim's grammar book. I memorized all the sentences in the book (in retrospect this wasn't a very efficient learning method).
 However, I feel that the memorization did make a lot of the grammar sink into my head, so I didn't have as much difficulty making sentences later on.
 After April, I stopped studying completely.&lt;/li&gt;
&lt;li&gt;2017-01 to 2017-09: Learned traditional Chinese characters (for Chinese).
 However, this helped my Japanese a lot because Japanese Kanji are more similar to traditional characters than simplified characters.&lt;/li&gt;
&lt;li&gt;2018-01 to 2018-08: start to watch a lot more anime to practice listening to Japanese.
 Specifically, I found that the website &lt;a href="https://animelon.com/"&gt;animelon&lt;/a&gt; was very helpful, since you can watch anime with both English and Japanese subtitles at the same time, so you can pause it and figure out what the characters were saying.
 Best part is that it is free!&lt;/li&gt;
&lt;li&gt;2018-06 to 2018-08: Started doing a lot of Japanese vocabulary flashcards.
 I learned about 2000 words in this time, according to Anki, however the actual 
 number is probably less, since many of those words were loanwords from English that
 I didn't really need to learn.
According to my Anki decks, when going to Japan, I had spent:&lt;/li&gt;
&lt;li&gt;70 hours studying my Japanese decks&lt;/li&gt;
&lt;li&gt;27 hours studying traditional characters&lt;/li&gt;
&lt;li&gt;156 hours studying simplified characters
Of course, outside of Anki I probably spent a lot more time than this, but this gives a lower bound for the time doing "traditional studying" 
(as opposed to watching content or reading books).&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Trip to Japan&lt;/h3&gt;
&lt;p&gt;I was in Japan for 2 weeks at the end of August&lt;sup id="fnref:3"&gt;&lt;a class="footnote-ref" href="https://austintripp.ca/blog/2018/09/19/japanese-learning-reflections/#fn:3"&gt;3&lt;/a&gt;&lt;/sup&gt;.
During this time, I tried to use Japanese as much as possible.
Most of my interactions with Japanese people were from talking to service people.
I tried to talk to every waiter, cashier, and ticket booth worker in Japanese.
However, these conversations tended to be very simple. For example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Cashier: Dozo! (&lt;em&gt;welcome!&lt;/em&gt;)&lt;/li&gt;
&lt;li&gt;Me: Konnichiwa! (&lt;em&gt;good day&lt;/em&gt;)&lt;/li&gt;
&lt;li&gt;Cashier: Konnichiwa! (&lt;em&gt;good day&lt;/em&gt;) [proceeds to scan items]&lt;/li&gt;
&lt;li&gt;Cashier: [some number] en desu (&lt;em&gt;this many yen&lt;/em&gt;)&lt;/li&gt;
&lt;li&gt;Me: [hands cashier money]&lt;/li&gt;
&lt;li&gt;Cashier: Arigatou gozaimasu! (&lt;em&gt;thanks&lt;/em&gt;)&lt;/li&gt;
&lt;li&gt;Me: Arigatou gozaimasu! (&lt;em&gt;thanks&lt;/em&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Obviously it doesn't take much skill to have this kind of conversation.
However, I was able to ask a few more complicated things to people:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;When buying a dessert, I told the cashier that I have a nut allergy, 
 and asked them to read the ingredients to make sure it didn't have nuts in it.
 However, I did a lot of research asking how to say this because obviously my allergies are very important for me, so that doesn't make this as impressive as it might normally be.&lt;/li&gt;
&lt;li&gt;At Tsujiki fish market, I bought raw shrimp from one of the vendors who spoke no English at all.
 I asked if it was raw, and he said yes (I had to look up the word for raw though). I asked him how one eats a raw shrimp, 
 so he ripped the tails off and gave it to us.&lt;/li&gt;
&lt;li&gt;I saw a vendor at the fish market with the character 鲸, which means whale.
 I asked if what he was serving was really whale, and he said yes 
 (I had to look up the word for whale).&lt;/li&gt;
&lt;li&gt;My friend lost his wallet, so I was able to go to the police station and tell them that he lost his wallet&lt;sup id="fnref:4"&gt;&lt;a class="footnote-ref" href="https://austintripp.ca/blog/2018/09/19/japanese-learning-reflections/#fn:4"&gt;4&lt;/a&gt;&lt;/sup&gt;.
 I had to look up the word for wallet though.&lt;/li&gt;
&lt;li&gt;I was able to ask a waiter in a restaurant with no English menu what the most popular dish was.
 He pointed to a particular soba dish, and so I told him we all wanted that.&lt;/li&gt;
&lt;li&gt;I was able to ask various stores if they had merchandise for specific animes 
 (it was for a friend, I swear).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;That being said, I also had a lot of failures, such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In one of the police boxes, when the policeman said "You don't need to come asking about the wallet, we will just call you",
 I was almost completely unable to say "We are tourists, and don't have a phone number.
 The last police box we visited told us just to come back to a police box to ask about the status instead of waiting for a call".
 Instead, he had to put me on the phone with an English-speaking colleague.&lt;/li&gt;
&lt;li&gt;A few times asking for directions, I got completely lost when the other person spoke, especially if the destination was more than a few turns away.&lt;/li&gt;
&lt;li&gt;After Typhoon Jebi caused a lot of damage to the Kansai region, I had a &lt;em&gt;really&lt;/em&gt;
 difficult time reading the websites of various attractions to check whether they were
 closed due to typhoon damages.
 Most of the time the announcements were never made on the English version of the websites, and the Japanese was written in a very polite and verbose manner, so I couldn't tell if it said "open" or "not open".&lt;/li&gt;
&lt;li&gt;When we were at an arcade using the "UFO Catcher" machines (the ones where you grab a prize with a crappy claw),
 my friend and I took too long to press the button, so it timed out and reset, so we lost our dollar.
 I tried to explain this to the owner of the shop, but he completely didn't understand what I meant (even when I used Google Translate!).
 It felt pretty embarrassing.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Reflection&lt;/h3&gt;
&lt;p&gt;I was slightly disappointed with my level of Japanese on the trip.
I think this was because I had studied so much vocabulary, but often found myself
unable to form a medium-difficulty sentence (like the police box one above).
I think it would have helped significantly if I had chosen to do some language exchanges beforehand.&lt;/p&gt;
&lt;p&gt;One thing I was relatively pleased with was my listening comprehension ability.
Obviously I was unable to understand people talking quickly about complicated things
(such as train announcers, commercials, and random people talking on the street),
but I was usually able to understand what people said when they spoke to me directly.
I think this came from watching anime, which gave me a lot of listening practice.&lt;/p&gt;
&lt;p&gt;I think that in the future, if I learn languages for travelling again, I should focus more on learning essential phrases instead of trying to get a very large vocabulary.
This will make it easier for me to have the low-level conversations I had with service people already, but require a lot less effort than what I did.&lt;/p&gt;
&lt;h3&gt;Summary&lt;/h3&gt;
&lt;p&gt;Overall, I think this is a summary of my progress:
- Did extensive preparation for trip to Japan
- Most interactions ok, however there were some notable failures
- Should have practiced speaking more
- For future trips, I should spend more time learning essential phrases&lt;/p&gt;
&lt;h3&gt;Future Plans&lt;/h3&gt;
&lt;p&gt;My next visits will likely be Korea/China, in summer 2019.
I feel pretty confident about using Chinese, but for Korean, I want to reach a basic-intermediate level (around B1),
with the ability to confidently handle everyday situations.
So far, I think &lt;a href="https://mangolanguages.com/"&gt;MangoLanguages&lt;/a&gt; (which I get for free from my local library)
is a good resource for learning basic phrases for daily life.
I will try to finish their Korean course before going there; something I wish I did for Japanese before going to Japan.&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;technically, it is a syllabary not an alphabet. &lt;a class="footnote-backref" href="https://austintripp.ca/blog/2018/09/19/japanese-learning-reflections/#fnref:1" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:2"&gt;
&lt;p&gt;this is an &lt;em&gt;excellent&lt;/em&gt; guide to learning Japanese. I really like his explanations. Best of all, it is free! (I don't believe you should have to pay to learn a language) &lt;a class="footnote-backref" href="https://austintripp.ca/blog/2018/09/19/japanese-learning-reflections/#fnref:2" title="Jump back to footnote 2 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:3"&gt;
&lt;p&gt;if you are planning a trip, note that August in Japan is &lt;em&gt;really&lt;/em&gt; hot and humid. Also there are typhoons, which I got to experience. &lt;a class="footnote-backref" href="https://austintripp.ca/blog/2018/09/19/japanese-learning-reflections/#fnref:3" title="Jump back to footnote 3 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:4"&gt;
&lt;p&gt;the Japanese police are &lt;em&gt;super&lt;/em&gt; polite, and seemed really eager to help. This is very different from Canadian/American police, who generally tend to sound  a bit annoyed when you ask them things. &lt;a class="footnote-backref" href="https://austintripp.ca/blog/2018/09/19/japanese-learning-reflections/#fnref:4" title="Jump back to footnote 4 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description><guid>https://austintripp.ca/blog/2018/09/19/japanese-learning-reflections/</guid><pubDate>Tue, 18 Sep 2018 23:00:00 GMT</pubDate></item><item><title>Sparse Matrices: 5 Tips and Tricks</title><link>https://austintripp.ca/blog/2018/09/12/sparse-matrices-tips1/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;p&gt;Over the course of my internship at the online shopping company &lt;a href="https://www.wish.com/"&gt;Wish&lt;/a&gt;, I have dealt a lot with a lot of data in the form of sparse matrices, specificaly in the form of item interaction matrices for customer data. In doing so, I have made heavy use of &lt;a href="https://docs.scipy.org/doc/scipy/reference/sparse.html"&gt;scipy's sparse matrices library&lt;/a&gt;.
Here are 5 tricks that I have learned.&lt;/p&gt;
&lt;p&gt;Note this post has an &lt;a href="https://github.com/AustinT/sparse-matrices-tips/blob/master/Sparse%20Matrix%20Tips%20and%20Tricks-%20Part%201.ipynb"&gt;associated Jupyter notebook&lt;/a&gt; that contains example code.&lt;/p&gt;
&lt;h3&gt;1: Use a normal dict instead of a &lt;code&gt;dok_matrix&lt;/code&gt; to construct sparse matrices incrementally&lt;/h3&gt;
&lt;p&gt;A dok matrix is essentially storing a sparse matrix in a hashmap. According to the &lt;a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.dok_matrix.html#scipy.sparse.dok_matrix"&gt;documentation&lt;/a&gt;, it is an efficient structure for constructing matrices incrementally, because adding an element to a hashmap is an $$O(1)$$ operation.
However, what the documentation doesn't mention is that the &lt;code&gt;dok_matrix&lt;/code&gt; class has a very significant overhead on item assignment. Suppose you have a dok matrix and you try to perform item assignment via:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;matrix[i, j] = some_value&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;This will end up calling the object's &lt;code&gt;__setitem__&lt;/code&gt; method, whose implementation can be seen &lt;a href="https://github.com/scipy/scipy/blob/552d5026754b5151eb5ae58b41f09e5b8ddcbde5/scipy/sparse/dok.py#L258-L308"&gt;here&lt;/a&gt;. Looking at the code for this method, there is a ton of type checking on the arguments, which is really inefficient if you know that your arguments are of the right type.&lt;/p&gt;
&lt;p&gt;A faster way to do it is to use an ordinary default dict, then directly update the underlying dictionary of the dok class to construct a dok matrix.&lt;/p&gt;
&lt;p&gt;If &lt;code&gt;matrix_dict&lt;/code&gt; is a default dict with (i, j) tuples as keys, and &lt;code&gt;matrix&lt;/code&gt; is a dok matrix, then the key line to do the update is:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;dict.update(matrix, matrix_dict)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;See this post's &lt;a href="https://github.com/AustinT/sparse-matrices-tips/blob/master/Sparse%20Matrix%20Tips%20and%20Tricks-%20Part%201.ipynb"&gt;notebook&lt;/a&gt; for more detail. I noticed a 20x speedup using this trick.&lt;/p&gt;
&lt;h3&gt;2: When using compressed sparse row (csr) format, it is faster to convert the whole matrix to compressed sparse column format and back than do a bunch of column selections&lt;/h3&gt;
&lt;p&gt;When dealing with a large co-purchase matrix, I came across this problem of having to zero out a bunch of columns to account for items that were missing in the next steps of my data analysis (I was trying to use the matrix to calculate a conditional distribution of item co-purchases). But with the csr format, changing this takes an incredibly long time! I found it was faster to convert to csc and back.&lt;/p&gt;
&lt;p&gt;However, it isn't always faster, as shown in my jupyter notebook. But is is always an option to consider. I've only seen it be faster in very sparse matrices (density about 1e-6), where I had to change about 10% of columns. For most use cases, it tends to be slower.&lt;/p&gt;
&lt;h3&gt;3: Don't be afraid to access the internal structure of csr/csc matrices&lt;/h3&gt;
&lt;p&gt;Calling &lt;code&gt;getrow()&lt;/code&gt; will make a copy of the data, which is obviously slower (&lt;a href="https://github.com/scipy/scipy/blob/f4a81d908031ade435f321de7fe85ad5576e931e/scipy/sparse/csr.py#L367"&gt;see code&lt;/a&gt;) 
It is much faster to access the underlying arrays yourself.
See the notebook for an example.&lt;/p&gt;
&lt;h3&gt;4: When doing adding operations, convert to csr format, since scipy does it internally anyways&lt;/h3&gt;
&lt;p&gt;Looking at &lt;a href="https://github.com/scipy/scipy/blob/552d5026754b5151eb5ae58b41f09e5b8ddcbde5/scipy/sparse/base.py#L328-L757"&gt;the base file for sparse matrices&lt;/a&gt;, 
a lot of the implementations of basic functions (e.g. min, multiply, equality checking) work by first converting to a csr matrix. 
So if you are doing these operations you might as well be in csr format to begin with. &lt;/p&gt;
&lt;h3&gt;5: NEVER convert from CSR to DOK format with a large matrix- use COO instead&lt;/h3&gt;
&lt;p&gt;Converting to a COO format is fast, and consists of just a few array swaps (see &lt;a href="https://github.com/scipy/scipy/blob/f1251aa680623c20efa95a530a8b02e729f6d8d9/scipy/sparse/compressed.py#L931-L940"&gt;source code&lt;/a&gt;).
Converting to a dok matrix on the other hand is &lt;em&gt;extremely&lt;/em&gt; inefficient, mainly because it &lt;em&gt;converts to a COO matrix anyways as an intermediate step&lt;/em&gt;! (&lt;a href="https://github.com/scipy/scipy/blob/f1251aa680623c20efa95a530a8b02e729f6d8d9/scipy/sparse/base.py#L891-L897"&gt;source&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Experimentally, I found converting to a COO matrix to be about 100x faster than converting to DOK with a large number of samples, and about twice as fast with a small number of samples.&lt;/p&gt;</description><guid>https://austintripp.ca/blog/2018/09/12/sparse-matrices-tips1/</guid><pubDate>Tue, 11 Sep 2018 23:00:00 GMT</pubDate></item><item><title>Why you should never be certain of your beliefs: a Bayesian perspective</title><link>https://austintripp.ca/blog/2018/08/21/bayes-no-certain-priors/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;p&gt;People are notoriously bad at estimating their percent confidence in their beliefs, as explained further in this &lt;a href="https://en.wikipedia.org/wiki/Overconfidence_effect"&gt;Wikipedia article&lt;/a&gt;. 
Something I thought of recently is what effect this overconfidence has from a Bayesian perspective. 
After a bit of math, I came to the conclusion that having extreme confidence in your beliefs (0% or 100% confidence) implies that you would be unable to change your beliefs if shown evidence to the contrary.
I believe this simple argument suggests that it is very irrational to hold prior beliefs of 0 or 100%. If you do feel this way, then you should choose a very high value (99.99%) or a very low value (0.001%), but always leave some room for error.&lt;/p&gt;
&lt;h3&gt;The model&lt;/h3&gt;
&lt;p&gt;Suppose you have a belief about a fact, \(f\), which for simplicity we will assume is a binary variable (either true or false). 
For example, \(f\) could be the statement "the earth is flat". 
Furthermore, suppose you can make some kind of observation, \(o\), that is affected by \(f\). 
In this example, \(o\) could be the observation of ships never disappearing over the horizon, which would be a natural consequence if the earth were flat.
From a Bayesian perspective, this observation can be defined as a conditional distribution. That is, the value of \(o\) will change based on the true value of \(f\). 
In our case, we could write \(p(o \mid f=\text{true}) = 1\), assuming here that the earth being flat will cause us to observe this effect with certainty.
On the other hand, if \(f\) is false, then it might still be possible to observe \(o\), but much less likely. We will say that \(p(o \mid f=\text{false}) = \epsilon \gt 0\), without deciding on a specific value for \(\epsilon\).&lt;/p&gt;
&lt;p&gt;Now suppose you observe a ship that doesn't disappear over the horizon: what does this mean about the earth being flat? From Bayes rule, we want to calculate:&lt;/p&gt;
&lt;p&gt;$$p(f\mid o) = \frac{p(o\mid f)p(f)}{p(o)}$$&lt;/p&gt;
&lt;p&gt;In other words, we can calculate the probability of the fact \(f\), just by knowing 3 things:
1. The effect of \(f\) on \(o\), \(p(o \mid  f) = 1\)
2. Our believe about \(f\) prior to our observation, \(p(f)\)
3. The probability of our observation, \(p(o)\)&lt;/p&gt;
&lt;p&gt;The goal of setting this up in a Bayesian context is to update our belief about \(f\), given that we have now made an observation. 
One strategy is to try to select a new value of \(f\) that is most likely given what we have observed. 
This is known as &lt;em&gt;maximum likelihood estimation&lt;/em&gt; (MLE). 
One nice thing about MLE is that we don't have to calculate \(p(f \mid o)\): we just have to maximize it with respect to \(f\). 
This means that in the 3 terms above, item #3 doesn't matter at all, because it is constant with respect to \(f\). So we can instead update our beliefs based on maximizing the surrogate function,&lt;/p&gt;
&lt;p&gt;$$p(f \mid o) \propto p(o \mid f)p(f) = g(f)$$&lt;/p&gt;
&lt;p&gt;Since \(f\) is a binary variable, we just have to consider two options for the maximization:
1. \(f\) is true, so \(g(f) = p(o \mid f=\text{true})p(f=\text{true}) = 
1\times p(f=\text{true}) = p(f=\text{true})\)
2. \(f\) is false, so \(p(f\mid o) = p(o \mid f=\text{false})p(f=\text{false}) = 
\epsilon (1 - p(f=\text{true}))\) &lt;/p&gt;
&lt;h3&gt;The most important part&lt;/h3&gt;
&lt;p&gt;So, what we should believe about \(f\) depends largely on our prior belief about \(f\), \(p(f)\). 
Now suppose we have an extremely certain belief about \(f\), that is \(p(f) = 1\) or \(p(f) = 0\).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If \(p(f) = 0\), meaning that we are certain the fact is false, then from option 1 above, the probability of \(f\) being true is proportional \(p(f) = 0\), and from option 2 above \(f\) is false with probability proportional to \(\epsilon (1-0) = \epsilon \gt 0\), since \(\epsilon\) is a probability we assume to be non-zero. This means that \(f\) will always be more likely to be false, so &lt;em&gt;it would be impossible for our belief to change from false to true!&lt;/em&gt; &lt;/li&gt;
&lt;li&gt;If \(p(f) = 1\), meaning that we are certain the fact is true, then from option 1 above, the probability that \(f\) is true is proportional to 1, while the probability of \(f\) being false is proportional to \((1-1) = 0\). So option 1 will always be greater than 0, &lt;em&gt;meaning our belief will never change from false to true&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note that in both of the options above, &lt;em&gt;it is impossible for our beliefs about \(f\) to change, despite observing evidence that supports \(f\)&lt;/em&gt;. This is essentially the mathematical equivalent of being closed-minded.&lt;/p&gt;
&lt;p&gt;On the other hand, if \(p(f) \notin {0, 1}\), then it would be possible for either option 1 or 2 to be higher, depending on value of \(epsilon\)&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="https://austintripp.ca/blog/2018/08/21/bayes-no-certain-priors/#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt;. So, &lt;em&gt;it would be possible to change your beliefs&lt;/em&gt;.&lt;/p&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;In short, this shows that having a certain prior is effectively the same as being unwilling to change one's beliefs, should one encounter evidence which goes against them.
If you accept this as irrational, and abide by Bayesian statistics&lt;sup id="fnref:2"&gt;&lt;a class="footnote-ref" href="https://austintripp.ca/blog/2018/08/21/bayes-no-certain-priors/#fn:2"&gt;2&lt;/a&gt;&lt;/sup&gt;, then you &lt;em&gt;must&lt;/em&gt; accept it as irrational to hold beliefs with certainty. 
That is, Bayes theorem is a mathematical way to show that one can only have an open mind if one is somewhat unsure about something. 
It is because of this logic that I am careful nowadays to avoid saying I am certain about anything, both to not mislead others and to not convince myself I know more than I actually do.&lt;/p&gt;
&lt;h3&gt;PS&lt;/h3&gt;
&lt;p&gt;I found this cool website, which is a "prior belief calibrator": &lt;a href="http://confidence.success-equation.com/"&gt;http://confidence.success-equation.com/&lt;/a&gt;. I recommend you try it out, and see how accurate your confidence in facts is.&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;specifically, when \(\epsilon = \frac{p(f)}{1-p(f)}\), the \(f\) is equally likely to be true and false. If \(\epsilon\) is higher, g is maximized when \(f\) is false, and if it is lower, then \(g\) is maximized when \(f\) is true. So depending on the value of epsilon, your belief could change. &lt;a class="footnote-backref" href="https://austintripp.ca/blog/2018/08/21/bayes-no-certain-priors/#fnref:1" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:2"&gt;
&lt;p&gt;including the setup of this model, which is simplistic and could reasonably be disagreed with. &lt;a class="footnote-backref" href="https://austintripp.ca/blog/2018/08/21/bayes-no-certain-priors/#fnref:2" title="Jump back to footnote 2 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description><guid>https://austintripp.ca/blog/2018/08/21/bayes-no-certain-priors/</guid><pubDate>Mon, 20 Aug 2018 23:00:00 GMT</pubDate></item><item><title>An Overview of Gradient Boosting and Popular Libraries for it.</title><link>https://austintripp.ca/blog/2018/07/17/gradient-boosting/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;p&gt;Everybody doing machine learning wants the best models possible. 
The aim of this blog article is the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;To provide an introduction to the machine learning technique known as &lt;em&gt;boosting&lt;/em&gt;, and specifically &lt;em&gt;gradient boosting&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;To compare/contrast boosting with other ensemble methods, such as &lt;em&gt;bagging&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;To explain and compare several popular gradient boosting frameworks, specifically &lt;em&gt;XGBoost&lt;/em&gt;, &lt;em&gt;CatBoost&lt;/em&gt;, and &lt;em&gt;LightGBM&lt;/em&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;A brief introduction to ensemble models&lt;/h3&gt;
&lt;p&gt;Machine learning tasks can be framed as a function approximation task, where the goal is to approximate a function \(f(\vec{x})\) given a set of observations 
\(\lbrace \left(\vec x_i, f(\vec x_i) \right)\rbrace _{i=1:N}\).
The main difficulty is that \(f\) can be very complex, or potentially we might not have a lot of data. So a simple model (e.g. linear regression) might not be able to adequately model \(f\), and a sufficiently complex model would overfit the training set.&lt;/p&gt;
&lt;p&gt;This kind of scenario is perfect for an ensemble method: where multiple models are combined together to create one superior model. Two popular methods for this are called &lt;em&gt;bagging&lt;/em&gt; and &lt;em&gt;boosting&lt;/em&gt;.&lt;/p&gt;
&lt;h4&gt;Bagging&lt;/h4&gt;
&lt;p&gt;Personally, I find this method to be more intuitive. The intuition behind bagging is that different models tend to make different kinds of errors, so given many models, it is unlikely that they will all make a similar misclassification error. Therefore, you train multiple models, and to inference on a data point you inference on all the models and then average/vote on the result.&lt;/p&gt;
&lt;p&gt;However, how do you guarantee that all the models will make different kinds of errors? Most simple models are fit using deterministic algorithms, meaning that training multiple copies of the same model won't yield any different results. The answer for this is &lt;em&gt;bootstrapping&lt;/em&gt;, which is the statistical name for subsampling from your data. By training models on different subsets of your training set, each point is only seen by a fraction of the models, and therefore not all the models will be able to overfit to that point. Furthermore, since the different models will also all see different data &lt;em&gt;distributions&lt;/em&gt;, they will also be unable to precisely overfit to your training set distribution. The overall effect of this is that the different models you train will be significantly less correlated, making the above objective of independent errors more true.&lt;/p&gt;
&lt;p&gt;This technique is what gives &lt;em&gt;bagging&lt;/em&gt; its name: it stands for &lt;em&gt;"Bootstrap AGGregatING"&lt;/em&gt;. (I used to think it was called bagging because you had a "bag of models", but this is not the case).&lt;/p&gt;
&lt;p&gt;Moving up one level of abstraction, bagging can be thought of as a weighted averaging model: given the outputs of several models, the bagging model assigns a weight to each of the model's predictions and sums them to get a total prediction. Assuming the bagging method uses N models, an averaging bagging model assigns a weight of \(1/N\) to each sub-model, and a mode bagging model assigns a weight of \(1/N_{mode}\). &lt;/p&gt;
&lt;h4&gt;Boosting&lt;/h4&gt;
&lt;p&gt;Boosting differs from bagging in the fitting procedure, and in the goal of each model. In bagging, the goal is to train models independently so that they make less correlated errors. In boosting however, models are trained sequentially, with the goal of training each model being to correct for the errors made by the previous model.&lt;/p&gt;
&lt;p&gt;That is, if you have a model \(m_j(\vec{x})\) fit after \(j\) iterations, then the goal of boosting is to fit a model&lt;/p&gt;
&lt;p&gt;$$m_{j+1}(\vec{x}) = f(\vec{x}) - m_j(\vec{x})$$ &lt;/p&gt;
&lt;p&gt;This essentially means that you are fitting a new model, such that when it is added to the previous model, perfectly corrects for its mistakes. Intuitively, you are training a model that &lt;em&gt;boosts&lt;/em&gt; the performance of your previous model, hence the name boosting.&lt;/p&gt;
&lt;h3&gt;Gradient boosting&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Gradient boosting&lt;/em&gt; is a specific variety of boosting, with a slightly different objective. Consider the following scenario: you would like to fit your entire dataset, but there is a specific region which you care more about, so errors in that region are more problematic than errors in other regions. One example of this kind of scenario would be medical diagnosis, where a false negative (saying no disease when there is really a disease) can have severe consequences. In this case, trying to match every point equally would not be the best choice.&lt;/p&gt;
&lt;p&gt;Typically in this kind of scenario, a &lt;em&gt;loss function&lt;/em&gt; is used to capture the error preferences in a single scalar value. In this context, the goal of adding another model to your set of models is not to decrease the fitting errors, but to decrease the loss function. Specifically, you would like it so that:&lt;/p&gt;
&lt;p&gt;$$L(m_{j+1} + m_j) \leq L(m_{j+1})$$&lt;/p&gt;
&lt;p&gt;One way to do this is to make \(m_{j+1}\) proportional to the loss gradient. By making \(m_{j+1} = \nabla_{m_j} L(m_j)\), we know that adding (subtracting) \(m_{j+1}\) will cause an increase (decrease) in the loss function, because the gradient gives the direction of steepest increase.&lt;/p&gt;
&lt;p&gt;However, because the gradient might vary significantly within a short distance, we multiply the gradient by a small constant to improve the accuracy of this first order approximation. &lt;/p&gt;
&lt;p&gt;So in summary, the difference between gradient boosting and normal boosting is the objective function. In normal boosting, we want:&lt;/p&gt;
&lt;p&gt;$$m_{j+1}(\vec{x}) = f(\vec{x}) - m_j(\vec{x})$$&lt;/p&gt;
&lt;p&gt;Whereas in gradient boosting, we want:&lt;/p&gt;
&lt;p&gt;$$m_{j+1} = \nabla _{m_j} L(m_j)$$&lt;/p&gt;
&lt;p&gt;Note that like bagging, since the outputs of different trees are being added to produce the final output, it can also be thought of as a weighted averaging model.&lt;/p&gt;
&lt;h3&gt;Gradient boosting and decision trees&lt;/h3&gt;
&lt;p&gt;Usually when gradient boosting is discussed, it is almost always in relation to decision trees. Why is that? While I am not 100% sure of this, I can think of a few main reasons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;With linear models (i.e. linear regression), the models are already additive, so adding two linear models is always equivalent to fitting a single linear model&lt;/li&gt;
&lt;li&gt;The gradient might be very high for a few training examples, and low for the rest (e.g. if only a few examples are misclassified). With most models, changing the parameters will affect all of the results, whereas with decision trees you can simply add a new leaf node to an existing leaf to further fit those specific examples, allowing you to increase accuracy for those examples while not sacrificing it for other examples.&lt;/li&gt;
&lt;li&gt;There are nice analytic updates about whether to split a node in a decision tree, while for other models (e.g. neural nets) the fitting takes a long time.&lt;/li&gt;
&lt;li&gt;Newer models (neural nets) are usually pretty strong learners, and in theory if the neural network consistently performs poorly on a specific output you can just train on that output more, or add more parameters to increase model complexity. So gradient boosting doesn't seem very necessary for this.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So while it seems like gradient boosting can be used for anything, in practice it is mainly used for decision tree models.&lt;/p&gt;
&lt;h3&gt;High-level comparison of gradient boosting frameworks&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;XGBoost&lt;/em&gt;, &lt;em&gt;CatBoost&lt;/em&gt;, and &lt;em&gt;LightGBM&lt;/em&gt; are all popular gradient boosting frameworks for decision trees. They all use essentially the same algorithm for one part of it, with the difference being in the other part. The two steps for most gradient boosting algorithms are:
1. An algorithm to produce candidate new trees, usually iteratively, by given a starting tree, by selecting a leaf node to split and performing a split. Usually the first tree in this algorithm is just a single leaf node (i.e. all items in one node)
2. An algorithm to evaluate the quality of these splits.&lt;/p&gt;
&lt;p&gt;The common part is usually #2, the evaluation algorithm, since this is usually just the loss reduction caused by the split. The frameworks mainly differ in how they reduce the computationally hard problem of producing candidate trees. Roughly speaking, these are how the above 3 algorithms work:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;XGBoost&lt;/em&gt;: Given an existing tree, the naive way to improve it would be to iterate over every possible split value for every possible feature, and calculate the loss for each split, then choose the best split. Note that iterating over every possible split realistically just means iterating over every feature value for every data point, since the only goal is partitioning your set of training data, meaning that two splits that yield different partitions are not useful.&lt;br&gt;
 XGBoost notes that this iteration isn't efficient for large datasets, so they portion the dataset into quantiles for each feature, and do their partitions based on the quantiles. This results in a more efficient, but still slightly brute force method.
 &lt;a href="https://arxiv.org/abs/1603.02754"&gt;This is the link to the XGBoost paper that explains it in more detail&lt;/a&gt; &lt;/li&gt;
&lt;li&gt;&lt;em&gt;CatBoost&lt;/em&gt;: one of the main ideas behind CatBoost is that if a data point is used to produce a model, then boosting it using the same data gives a biased estimate of the gradient (biased with respect to the underlying data distribution). They counteract this by training an ensemble of models based on different permutations of the data, where each model is boosted using only new data points (i.e. data points it hasn't seen before). The multiple permutations ensure that each data point has the chance to be used for a &lt;em&gt;good&lt;/em&gt; model, since the first few points in a given permutation are going to be used to boost models trained on &lt;em&gt;even fewer points&lt;/em&gt;, so the model will be bad.&lt;br&gt;
 In addition, CatBoost also has some unique ways of handling categorical variables. &lt;a href="https://arxiv.org/abs/1706.09516"&gt;This is the CatBoost paper, which provides more detail&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;LightGBM&lt;/em&gt;: Microsoft's LightGBM does 2 things. Firstly, they throw away data points which have small gradients, reducing the amount of data that they have to calculate splits over (this is done heuristically). Secondly, they bundle features which are mutually exclusive, effectively taking two features X and Y and doing computations with a new feature, (X or Y). This is also estimated heuristically. &lt;a href="https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree"&gt;See the original paper for more details&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So in summary, the main difference between these libraries is how they choose to reduce the number of splits that they investigate, and which points they choose to make/evaluate those splits. But the underlying split evaluation idea is almost identical.&lt;/p&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;In summary, I've explained what bagging and boosting are, and some of the key gradient boosting libraries. Even though boosting techniques don't seem very prominent in modern machine learning literature, I think they are still valuable to know. They are especially useful for serving machine learning models at scale, where simple models like decision trees have huge speed advantages on large datasets. So I believe that boosting/gradient boosting will prove useful well into the future.&lt;/p&gt;</description><guid>https://austintripp.ca/blog/2018/07/17/gradient-boosting/</guid><pubDate>Mon, 16 Jul 2018 23:00:00 GMT</pubDate></item><item><title>Turning Adam Optimization into SGD</title><link>https://austintripp.ca/blog/2018/07/02/adam-to-sgd/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;h3&gt;Motivation&lt;/h3&gt;
&lt;p&gt;This strange question came up when working on a machine learning project to generate embeddings. 
Working with the version of Pytorch available on our DGX (similar to version 0.3.1), I found there was an optimizer called &lt;em&gt;SparseAdam&lt;/em&gt; but not one called &lt;em&gt;SparseSGD&lt;/em&gt;.
Since what I really wanted to do was use SGD, I wondered: could I turn the Adam optimizer into an SGD optimizer by setting the hyperparameters \(\beta_1\), \(\beta_2\), and \(\epsilon\)?&lt;/p&gt;
&lt;h3&gt;The Answer&lt;/h3&gt;
&lt;p&gt;Probably not. Looking at the &lt;a href="https://arxiv.org/abs/1412.6980"&gt;original paper for Adam&lt;/a&gt;, the formula for the parameter updates is:&lt;/p&gt;
&lt;p&gt;$$\theta_{t+1} = \theta_t - \alpha * \frac{\hat{m}}{\sqrt{\hat{v}_t}+\epsilon}$$&lt;/p&gt;
&lt;p&gt;To make this equal to gradient descent, we need the second term to equal the gradient.&lt;/p&gt;
&lt;p&gt;Luckily, \(m_t\) is directly related to the gradient, via the equation:&lt;/p&gt;
&lt;p&gt;$$m_t = \beta_1 m_{t-1} + ( 1- \beta_1) \nabla \theta_t$$&lt;/p&gt;
&lt;p&gt;$$\hat{m_t} = \frac{m_t}{1-\beta_1^t}$$&lt;/p&gt;
&lt;p&gt;Clearly, setting \(\beta_1=0\) will set the value to the gradient value. Note that this will also mean the normalization doesn't change \(m_t\).&lt;/p&gt;
&lt;p&gt;The problem is the term \(\hat{v}_t\), defined as:&lt;/p&gt;
&lt;p&gt;$$v_t = \beta_2 * v_{t-1} + (1-\beta_2) (\nabla \theta_t)^2$$&lt;/p&gt;
&lt;p&gt;$$\hat{v_t} = \frac{v_t}{1-\beta_2^t}$$&lt;/p&gt;
&lt;p&gt;We want this term to equal 1, to disappear from the fraction. However, setting \(\beta_2=0\) will cause it to be proportional to the square of the gradient, and setting \(\beta_2 = 1\) will cause a division by 0 error in the normalization. So because of this, I don't see a way to convert Adam into SGD. The gradient normalization is just build in too much into the algorithm.  &lt;/p&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;I don't think it is possible. And after reading the docs again, SGD is already compatible with sparse matrices, so this was completely unnecessary. It was a fun thought exercise though.&lt;/p&gt;</description><guid>https://austintripp.ca/blog/2018/07/02/adam-to-sgd/</guid><pubDate>Sun, 01 Jul 2018 23:00:00 GMT</pubDate></item><item><title>Paper Review: A Computational Approach to Organizational Structure</title><link>https://austintripp.ca/blog/2018/07/01/organization_structure_paper/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;h3&gt;Motivation&lt;/h3&gt;
&lt;p&gt;Ever since I've started doing internships, the concept of efficient organizations has piqued my interest. In every workplace I have been in, time is always wasted by inefficient transfers of information. For example, long meetings where most of the content is irrelevant to most of the people, or repeated interactions with co-workers where you explain the same thing to all of them. Assuming employees make ~$40/hour, a 1 h meeting with 100 people will cost $4000! If these meetings are not productive, then the company gets a negative return on its time investment, which essentially means the company is wasting money. Clearly organizational efficiency is a financially important objective.&lt;/p&gt;
&lt;p&gt;Much of an organization's efficiency can be linked to its structure. Long, big meetings are usually a consequence of a strong &lt;em&gt;hierarchical&lt;/em&gt; organization, where work is done by employees, then synced to a centralized node (a boss), and then possibly recapitulated to the workers in a meeting. However, are there better ways to structure an organization that would save time?&lt;/p&gt;
&lt;h3&gt;The Paper&lt;/h3&gt;
&lt;p&gt;The paper &lt;a href="https://arxiv.org/abs/1806.05701"&gt;A Computational Approach to Organizational Structure&lt;/a&gt; tries to address this problem from a fundamental perspective. They frame the problem as such:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Model an organization as a &lt;em&gt;graph&lt;/em&gt; with the nodes being people, and the edges being connections between them. For example, a hierarchical organization would be a tree&lt;/li&gt;
&lt;li&gt;Model tasks as &lt;em&gt;tokens&lt;/em&gt;, where each node (person) has tokens which they can do the following with:&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Compute&lt;/em&gt; with two tokens, to form one token&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Transmit&lt;/em&gt; one token to a neighbouring node
 These actions are assumed to take \(t_c\) and \(t_m\) respectively.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As a sample problem, the paper chooses to analyze the &lt;em&gt;information aggregation&lt;/em&gt; problem, which means taking all the tokens (tasks) and combining them at one node. This is equivalent to passing around information: for example, doing an analysis of which items to purchase for the company, then condensing and passing the recommendations to the CEO to make the final decision.&lt;/p&gt;
&lt;h3&gt;Their algorithm&lt;/h3&gt;
&lt;p&gt;The paper presents 2 things:
- An exact, polynomial time algorithm for solving the problem on fully-connected graphs
- An approximate algorithm for solving it on arbitrary graphs.&lt;/p&gt;
&lt;p&gt;The exact algorithm is essentially a hierarchical approach: given a tree, the leaf nodes pass their tokens to their parents, who compute it, and pass the tokens to their parents, etc. They give a formula for designing the optimal tree recursively. To make this work on an arbitrary graph, simply embed the tree in the graph. I talk about this algorithm in more detail &lt;a href="https://austintripp.ca/blog/2018/07/01/organization_structure_paper/#more-detailed-explanation-of-the-fully-connected-graph-optimization"&gt;below&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The approximate algorithm is similar in intuition. Designate some nodes as sources, and the rest as sinks. Then pass tokens from sources to sinks, which then aggregate the tokens together.&lt;/p&gt;
&lt;p&gt;The bulk of the paper is essentially dense math proving that these solutions run in polynomial time. &lt;/p&gt;
&lt;h3&gt;My thoughts on the paper&lt;/h3&gt;
&lt;p&gt;I won't lie: I didn't read the proofs for this. But the intuition was very interesting. Essentially the result of the paper supported a hierarchical information transfer process, at least for centralized transfer of information. I think it would be interesting to consider the downwards transfer of information too, or modelling a multi-step decision process with feedback.&lt;/p&gt;
&lt;p&gt;Another analogous problem that I think is interesting not only propagating information to a central source, but as work gets done, propagating information on that work to all other nodes (i.e. keep people up to date on what other people are doing). This is another problem I have observed in big companies. The analogous idea here would probably be each computation generating its own mini-token, which should be processed and passed around to all other nodes in the network (in some form).  &lt;/p&gt;
&lt;h3&gt;More detailed explanation of the fully-connected graph optimization&lt;/h3&gt;
&lt;p&gt;I found this quite interesting, so I thought I would elaborate more on it. The key formula is defining a tree \(T(R)\) as the largest tree such that information can be greedily aggregated within R time steps. The formula for such a tree is:&lt;/p&gt;
&lt;p&gt;$$T(R) = \begin{cases}
\text{Single leaf},  &amp;amp; \text{if $R &amp;lt; t_m &amp;lt; t_c$} \
T(R-t_c) \text{ with child } T(R-t_c-t_m), &amp;amp; \text{otherwise}
\end{cases}$$ &lt;/p&gt;
&lt;p&gt;This can be interpreted in a simple way: if a node  has a child, then it will take \(t_m\) time to get the token from the child node, and \(t_c\) to compute it. So if \(R &amp;lt; t_m + t_c\), then there is no way to complete the computation in \(R\) rounds. So the tree cannot have any leaves.&lt;/p&gt;
&lt;p&gt;Otherwise, the tree can have leaves. So the largest tree you can create is the largest tree you can make and still have enough rounds left for one computation (\(T(R-t_c)\)), with an additional leaf tree which can be fully computed, and still have time to pass its message to the root (\(T(R-t_c-t_m)\)).&lt;/p&gt;
&lt;p&gt;I really liked this formula because it was so clean and recursive.&lt;/p&gt;</description><guid>https://austintripp.ca/blog/2018/07/01/organization_structure_paper/</guid><pubDate>Sat, 30 Jun 2018 23:00:00 GMT</pubDate></item><item><title>How to guess a Kanji's on-yomi in 4 easy steps</title><link>https://austintripp.ca/blog/2018/06/25/onyomi/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;p&gt;Lately I have been putting a lot of effort into studying Japanese, to prepare for my upcoming trip to Japan at the end of summer 2018. While learning the readings for various kanji (Chinese character) words, I've noticed that a lot of their pronunciations (&lt;em&gt;on-yomi&lt;/em&gt;) are related to the Chinese pronunciations in an interesting way. Let me explain my empirical theory about how to to convert Chinese pronunciations into Japanese ones:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;b&gt;Say no:&lt;/b&gt; No to too many letters. If the Chinese word has multiple consonants, or too many vowels, drop them.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Change the vowels (and sometimes the consonants:&lt;/b&gt; Otherwise it would be too easy.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Go K-razy:&lt;/b&gt; Find ways to put the letter K into the word as much as possible. For some reason it seems like all kanji have lots of k's in them.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;(Optional) Add some extra syllables:&lt;/b&gt; Ususally either つ (tsu) or く (ku)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Sound weird? Let me walk you through some examples.&lt;/p&gt;
&lt;h3&gt;Some examples&lt;/h3&gt;
&lt;h4&gt;結果 [&lt;em&gt;jié guǒ&lt;/em&gt;] (result)&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;Say no: the &lt;em&gt;ie&lt;/em&gt; and &lt;em&gt;uo&lt;/em&gt; are clusters of vowels, so we say no to that, and remove a vowel from each. So we get &lt;em&gt;ji'go&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Change the vowels: change i to e, and o to a: so we get &lt;em&gt;je'ga&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Go K-razy: now change all consonants to the letter k. Also add another k into the middle of the word. So we get &lt;em&gt;kek'ka&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;Skip this step&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;So we get the final Japanese pronunciation, &lt;em&gt;kekka&lt;/em&gt; (けっか). Wow that word has a lot of k's.&lt;/p&gt;
&lt;h4&gt;計画 [&lt;em&gt;jì huà&lt;/em&gt;] (plan/project)&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;Say no: the &lt;em&gt;ua&lt;/em&gt; in &lt;em&gt;hua&lt;/em&gt; just becomes a.&lt;/li&gt;
&lt;li&gt;Change the vowels: &lt;em&gt;ji&lt;/em&gt; becomes &lt;em&gt;jei&lt;/em&gt; (pronounced just like je but with a "long vowel")&lt;/li&gt;
&lt;li&gt;Go K-razy: Now make every consonant a K. It becomes &lt;em&gt;keika&lt;/em&gt;. Also, just add an extra k syllable on the end, &lt;em&gt;ku&lt;/em&gt;. So we get &lt;em&gt;keikaku&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This gives us &lt;em&gt;kei'kaku&lt;/em&gt; (けいかく), the Japanese pronunciation.&lt;/p&gt;
&lt;h4&gt;価格 [&lt;em&gt;jià gé&lt;/em&gt;] (price)&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;Say no: &lt;em&gt;jia&lt;/em&gt; becomes &lt;em&gt;ja&lt;/em&gt;, so we have &lt;em&gt;ja ge&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Change the vowels: &lt;em&gt;ge&lt;/em&gt; becomes &lt;em&gt;ga&lt;/em&gt;, so we have &lt;em&gt;ja ga&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Add K's. &lt;em&gt;ja&lt;/em&gt; becomes &lt;em&gt;ka&lt;/em&gt;, and &lt;em&gt;ga&lt;/em&gt; also becomes &lt;em&gt;ka&lt;/em&gt;. And why not, add an extra &lt;em&gt;ku&lt;/em&gt;, giving us &lt;em&gt;kakaku&lt;/em&gt;. Note that every consonant has now become the letter K.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We now have the Japanese pronunciation, &lt;em&gt;ka'kaku&lt;/em&gt; (かかく).&lt;/p&gt;
&lt;h3&gt;More examples of Kanji with K's in weird places or just lots of K's&lt;/h3&gt;
&lt;p&gt;So you've seen some examples worked through. Look at all those K's!! Although those were somewhat cherry-picked, allow me to give some more examples of on-yomi with tons of extra K's.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Kanji&lt;/th&gt;
&lt;th style="text-align: right;"&gt;Meaning&lt;/th&gt;
&lt;th style="text-align: center;"&gt;Chinese (no tones)&lt;/th&gt;
&lt;th style="text-align: right;"&gt;Japanese&lt;/th&gt;
&lt;th style="text-align: right;"&gt;# K's added&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;改革&lt;/td&gt;
&lt;td style="text-align: right;"&gt;Revolution&lt;/td&gt;
&lt;td style="text-align: center;"&gt;gai ge&lt;/td&gt;
&lt;td style="text-align: right;"&gt;kaikaku&lt;/td&gt;
&lt;td style="text-align: right;"&gt;4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;過去&lt;/td&gt;
&lt;td style="text-align: right;"&gt;past&lt;/td&gt;
&lt;td style="text-align: center;"&gt;guo qu&lt;/td&gt;
&lt;td style="text-align: right;"&gt;kako&lt;/td&gt;
&lt;td style="text-align: right;"&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;目的&lt;/td&gt;
&lt;td style="text-align: right;"&gt;goal&lt;/td&gt;
&lt;td style="text-align: center;"&gt;mu di&lt;/td&gt;
&lt;td style="text-align: right;"&gt;moku teki&lt;/td&gt;
&lt;td style="text-align: right;"&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;地下&lt;/td&gt;
&lt;td style="text-align: right;"&gt;basement&lt;/td&gt;
&lt;td style="text-align: center;"&gt;di xia&lt;/td&gt;
&lt;td style="text-align: right;"&gt;chi ka&lt;/td&gt;
&lt;td style="text-align: right;"&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;高校&lt;/td&gt;
&lt;td style="text-align: right;"&gt;high school&lt;/td&gt;
&lt;td style="text-align: center;"&gt;gao xiao&lt;/td&gt;
&lt;td style="text-align: right;"&gt;kou kou&lt;/td&gt;
&lt;td style="text-align: right;"&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;強化&lt;/td&gt;
&lt;td style="text-align: right;"&gt;strengthening&lt;/td&gt;
&lt;td style="text-align: center;"&gt;qiang hua&lt;/td&gt;
&lt;td style="text-align: right;"&gt;kyou ka&lt;/td&gt;
&lt;td style="text-align: right;"&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;教科&lt;/td&gt;
&lt;td style="text-align: right;"&gt;subject&lt;/td&gt;
&lt;td style="text-align: center;"&gt;jiao ke&lt;/td&gt;
&lt;td style="text-align: right;"&gt;kyou ka&lt;/td&gt;
&lt;td style="text-align: right;"&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;空港&lt;/td&gt;
&lt;td style="text-align: right;"&gt;airport&lt;/td&gt;
&lt;td style="text-align: center;"&gt;kong gang&lt;/td&gt;
&lt;td style="text-align: right;"&gt;kuu kou&lt;/td&gt;
&lt;td style="text-align: right;"&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;効果&lt;/td&gt;
&lt;td style="text-align: right;"&gt;effect&lt;/td&gt;
&lt;td style="text-align: center;"&gt;xiao guo&lt;/td&gt;
&lt;td style="text-align: right;"&gt;kou ka&lt;/td&gt;
&lt;td style="text-align: right;"&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;後悔&lt;/td&gt;
&lt;td style="text-align: right;"&gt;regret&lt;/td&gt;
&lt;td style="text-align: center;"&gt;hou hui&lt;/td&gt;
&lt;td style="text-align: right;"&gt;kou kai&lt;/td&gt;
&lt;td style="text-align: right;"&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;結婚&lt;/td&gt;
&lt;td style="text-align: right;"&gt;marriage&lt;/td&gt;
&lt;td style="text-align: center;"&gt;jie hun&lt;/td&gt;
&lt;td style="text-align: right;"&gt;kekkon&lt;/td&gt;
&lt;td style="text-align: right;"&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;国会&lt;/td&gt;
&lt;td style="text-align: right;"&gt;Japanese parliament&lt;/td&gt;
&lt;td style="text-align: center;"&gt;guo hui&lt;/td&gt;
&lt;td style="text-align: right;"&gt;kokkai&lt;/td&gt;
&lt;td style="text-align: right;"&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;This weird method works surprisingly well. Even though this is probably just a confirmation bias, now that I have this idea in mind I always think of Japanese pronunciations as variations on the Chinese pronunciation. And I really notice the large number of K's! These kind of weird things are part of the reason I like learning languages.&lt;/p&gt;</description><guid>https://austintripp.ca/blog/2018/06/25/onyomi/</guid><pubDate>Sun, 24 Jun 2018 23:00:00 GMT</pubDate></item></channel></rss>