<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Austin Tripp's website</title><link>https://austintripp.ca/</link><description>Austin Tripp's personal website</description><atom:link href="https://austintripp.ca/rss.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2025 &lt;a href="mailto:austin.james.tripp[at]gmail.com"&gt;Austin Tripp&lt;/a&gt; MIT License</copyright><lastBuildDate>Mon, 12 May 2025 08:59:57 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Chebyshev Scalarization Explained</title><link>https://austintripp.ca/blog/2025-05-12-chebyshev-scalarization/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;div&gt;&lt;p&gt;I've been reading about multi-objective optimization recently.&lt;sup id="fnref:surprise"&gt;&lt;a class="footnote-ref" href="https://austintripp.ca/blog/2025-05-12-chebyshev-scalarization/#fn:surprise"&gt;1&lt;/a&gt;&lt;/sup&gt;
Many papers state limitations of "linear scalarization" approaches,
mainly that it might not be able to represent all Pareto-optimal solutions
(if this sentence does not make sense to you, see &lt;a href="https://austintripp.ca/blog/2025-05-12-chebyshev-scalarization/#background-to-multi-objective-optimization"&gt;background&lt;/a&gt;).
Chebyshev scalarization is sometimes mentioned as an alternative which &lt;em&gt;can&lt;/em&gt; represent all solutions.
However, these papers mention it in passing without a proper explanation,
and I did not find a good explanation of it online.&lt;sup id="fnref:online"&gt;&lt;a class="footnote-ref" href="https://austintripp.ca/blog/2025-05-12-chebyshev-scalarization/#fn:online"&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;After doing a bit of my own research,&lt;sup id="fnref:gemini"&gt;&lt;a class="footnote-ref" href="https://austintripp.ca/blog/2025-05-12-chebyshev-scalarization/#fn:gemini"&gt;3&lt;/a&gt;&lt;/sup&gt; I found that Chebyshev scalarization is actually not too complicated, so I thought &lt;em&gt;I&lt;/em&gt; would explain it online.
In this post, I:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Give definitions for Chebyshev scalarization (for both maximization and minimization)&lt;/li&gt;
&lt;li&gt;Give a simple proof that it can represent all Pareto-optimal solutions&lt;/li&gt;
&lt;li&gt;Explain its relationship to linear scalarization via $\ell_p$ norms.&lt;/li&gt;
&lt;li&gt;Give a geometric interpretation via an interactive visualization&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="https://austintripp.ca/blog/2025-05-12-chebyshev-scalarization/"&gt;Read more…&lt;/a&gt; (13 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>bayesian optimization</category><category>machine learning</category><guid>https://austintripp.ca/blog/2025-05-12-chebyshev-scalarization/</guid><pubDate>Mon, 12 May 2025 00:00:00 GMT</pubDate></item><item><title>Reminder that Claude is really good for Chinese grammar</title><link>https://austintripp.ca/blog/2025-05-08-claude-chinese-grammar/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;div&gt;&lt;p&gt;This shouldn't surprise anybody who uses LLMs a lot, but LLMs are &lt;em&gt;really&lt;/em&gt; good
at common languages and translation. When practising Chinese today, a sentence
which confused me was:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://austintripp.ca/blog/2025-05-08-claude-chinese-grammar/"&gt;Read more…&lt;/a&gt; (1 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>chinese</category><category>language</category><category>using large language models</category><guid>https://austintripp.ca/blog/2025-05-08-claude-chinese-grammar/</guid><pubDate>Thu, 08 May 2025 00:00:00 GMT</pubDate></item><item><title>Some Beijing travel tips</title><link>https://austintripp.ca/blog/2025-04-28-beijing-travel/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;div&gt;&lt;p&gt;Earlier this year I went to Beijing. Here are some miscellaneous travel tips.
What I am writing here is probably not unique or original, treat this just as
my personal emphasis / endorsement.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://austintripp.ca/blog/2025-04-28-beijing-travel/"&gt;Read more…&lt;/a&gt; (1 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>travel</category><guid>https://austintripp.ca/blog/2025-04-28-beijing-travel/</guid><pubDate>Mon, 28 Apr 2025 00:00:00 GMT</pubDate></item><item><title>Taking the V out of VAEs: long live KL-regularized autoencoders!</title><link>https://austintripp.ca/blog/2025-04-24-v-out-of-vae/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;div&gt;&lt;p&gt;I recently read &lt;a href="https://sander.ai/2025/04/15/latents.html#"&gt;this post about generative modelling in latent
space&lt;/a&gt; by Sander Dieleman and
agreed strongly with the following quote (copied verbatim except for typo
correction):&lt;/p&gt;
&lt;p&gt;&lt;a href="https://austintripp.ca/blog/2025-04-24-v-out-of-vae/"&gt;Read more…&lt;/a&gt; (1 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>machine learning</category><guid>https://austintripp.ca/blog/2025-04-24-v-out-of-vae/</guid><pubDate>Thu, 24 Apr 2025 00:00:00 GMT</pubDate></item><item><title>Coding python packages with AI</title><link>https://austintripp.ca/blog/2025-04-12-ai-coding-python-package/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;div&gt;&lt;p&gt;I tried using some new LLM tools to code 2 entire python packages (instead of
editing a handful of lines at a time, which is what I did previously). It went
well! These tools are not perfect, but they are useful!&lt;/p&gt;
&lt;p&gt;&lt;a href="https://austintripp.ca/blog/2025-04-12-ai-coding-python-package/"&gt;Read more…&lt;/a&gt; (3 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>machine learning</category><category>using large language models</category><guid>https://austintripp.ca/blog/2025-04-12-ai-coding-python-package/</guid><pubDate>Sat, 12 Apr 2025 00:00:00 GMT</pubDate></item><item><title>Why your active learning algorithm may not do better than random</title><link>https://austintripp.ca/blog/2025-04-02-active-learning-random/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;div&gt;&lt;p&gt;I am a big fan of active learning, but I am also acutely aware of its potential
failure modes. A common failure mode is &lt;em&gt;random-like&lt;/em&gt; performance: achieving no
better "success"&lt;sup id="fnref:success"&gt;&lt;a class="footnote-ref" href="https://austintripp.ca/blog/2025-04-02-active-learning-random/#fn:success"&gt;1&lt;/a&gt;&lt;/sup&gt; in picking points than a &lt;em&gt;random&lt;/em&gt; policy. Indeed, it
is possible to experience this when the implementation is flawed.&lt;sup id="fnref:wrong"&gt;&lt;a class="footnote-ref" href="https://austintripp.ca/blog/2025-04-02-active-learning-random/#fn:wrong"&gt;2&lt;/a&gt;&lt;/sup&gt;
However, in some problems it &lt;em&gt;may not be possible&lt;/em&gt; to beat random-like
performance. In this post I try to explain why.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://austintripp.ca/blog/2025-04-02-active-learning-random/"&gt;Read more…&lt;/a&gt; (3 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>machine learning</category><category>speculation</category><guid>https://austintripp.ca/blog/2025-04-02-active-learning-random/</guid><pubDate>Wed, 02 Apr 2025 00:00:00 GMT</pubDate></item><item><title>Generic recommendations for cheminformatics models.</title><link>https://austintripp.ca/blog/2025-04-01-cheminformatics-model-qa/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;div&gt;&lt;p&gt;I was recently asked via email for my thoughts on which models to use &lt;em&gt;in
general&lt;/em&gt; for molecular property prediction. I'm sharing my responses publicly
in case they are useful.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://austintripp.ca/blog/2025-04-01-cheminformatics-model-qa/"&gt;Read more…&lt;/a&gt; (1 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>machine learning</category><category>speculation</category><guid>https://austintripp.ca/blog/2025-04-01-cheminformatics-model-qa/</guid><pubDate>Tue, 01 Apr 2025 00:00:00 GMT</pubDate></item><item><title>Using LLMs to improve my Chinese</title><link>https://austintripp.ca/blog/2025-03-30-llms-improve-chinese/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;div&gt;&lt;p&gt;I've been learning Chinese for almost 10 years now, but still make
awkward-sounding sentences when I speak. A few months ago I thought "why not
use LLMs to help me speak more naturally", and found that it does not take much
prompting to get useful feedback. Here is a conversation with Claude 3.5 from a
few months ago:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://austintripp.ca/blog/2025-03-30-llms-improve-chinese/"&gt;Read more…&lt;/a&gt; (1 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>chinese</category><category>language</category><category>using large language models</category><guid>https://austintripp.ca/blog/2025-03-30-llms-improve-chinese/</guid><pubDate>Sun, 30 Mar 2025 00:00:00 GMT</pubDate></item><item><title>Conceptual confusion about desirable outputs of reaction prediction models.</title><link>https://austintripp.ca/blog/2025-03-27-reaction-correctness/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;div&gt;&lt;p&gt;In the literature about machine learning for retrosynthesis, one line of work
tries to predict chemical reactions, either in the &lt;em&gt;forward direction&lt;/em&gt; (ie what
products will A + B form) or in the &lt;em&gt;backward direction&lt;/em&gt; (ie what reactants
could react to produce molecule C). Such models are often trained on datasets
of known reactions like Pistachio or USPTO, with the hope of generalizing to
new "correct" reactions. However, this formulation of the problem overlooks a
lot of subtleties about what makes a reaction "correct". In this post I will
present a more nuanced mental model which (hopefully) clarifies some
ambiguities.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://austintripp.ca/blog/2025-03-27-reaction-correctness/"&gt;Read more…&lt;/a&gt; (3 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>machine learning</category><category>retrosynthesis</category><category>speculation</category><guid>https://austintripp.ca/blog/2025-03-27-reaction-correctness/</guid><pubDate>Thu, 27 Mar 2025 00:00:00 GMT</pubDate></item><item><title>Punishing poor reviewers at CVPR</title><link>https://austintripp.ca/blog/2025-03-04-punish-bad-reviewers/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;p&gt;This year &lt;a href="https://cvpr.thecvf.com/"&gt;CVPR&lt;/a&gt; pledged to make all authors
participate in peer review, and reject papers from authors who wrote
low-quality reviews.&lt;sup id="fnref:ref"&gt;&lt;a class="footnote-ref" href="https://austintripp.ca/blog/2025-03-04-punish-bad-reviewers/#fn:ref"&gt;1&lt;/a&gt;&lt;/sup&gt; Last week they &lt;a href="https://x.com/CVPR/status/1894853624200863958"&gt;confirmed on
Twitter&lt;/a&gt; that they followed
through with this and rejected 19 papers. Presumably this is a tiny fraction of
the overall papers submitted, but I hope this is an effective deterrent for
future authors. At the very least, I'm glad that &lt;em&gt;some&lt;/em&gt; major conference tried
something like this!&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn:ref"&gt;
&lt;p&gt;&lt;a href="https://cvpr.thecvf.com/Conferences/2025/CVPRChanges"&gt;https://cvpr.thecvf.com/Conferences/2025/CVPRChanges&lt;/a&gt; &lt;a class="footnote-backref" href="https://austintripp.ca/blog/2025-03-04-punish-bad-reviewers/#fnref:ref" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description><category>machine learning</category><category>opinion</category><category>peer review</category><guid>https://austintripp.ca/blog/2025-03-04-punish-bad-reviewers/</guid><pubDate>Tue, 04 Mar 2025 00:00:00 GMT</pubDate></item></channel></rss>