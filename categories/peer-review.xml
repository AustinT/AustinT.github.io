<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Austin Tripp's website (Posts about peer review)</title><link>https://austintripp.ca/</link><description></description><atom:link href="https://austintripp.ca/categories/peer-review.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2026 &lt;a href="mailto:austin.james.tripp[at]gmail.com"&gt;Austin Tripp&lt;/a&gt; MIT License</copyright><lastBuildDate>Sun, 04 Jan 2026 14:41:44 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Finally, an ML conference review guide!</title><link>https://austintripp.ca/blog/2025-11-04-finally-a-reviewer-guide/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;div&gt;&lt;p&gt;ICLR 2026 released a detailed review guide as part of its review process
(&lt;a href="https://iclr.cc/Conferences/2026/ReviewerGuide"&gt;link&lt;/a&gt;). Let's analyze it!&lt;/p&gt;
&lt;p&gt;&lt;a href="https://austintripp.ca/blog/2025-11-04-finally-a-reviewer-guide/"&gt;Read more…&lt;/a&gt; (3 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>machine learning</category><category>peer review</category><guid>https://austintripp.ca/blog/2025-11-04-finally-a-reviewer-guide/</guid><pubDate>Tue, 04 Nov 2025 00:00:00 GMT</pubDate></item><item><title>My review guide for machine learning conference papers</title><link>https://austintripp.ca/blog/2025-06-22-ml-conference-review-guide/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;div&gt;&lt;p&gt;There is no official step by step guide for how to review ML conference papers
at venues like NeurIPS/ICML/ICLR.&lt;sup id="fnref:prev"&gt;&lt;a class="footnote-ref" href="https://austintripp.ca/blog/2025-06-22-ml-conference-review-guide/#fn:prev"&gt;1&lt;/a&gt;&lt;/sup&gt; In this post, I try to explain &lt;em&gt;my
guide&lt;/em&gt;. It is not official, endorsed, or necessarily good, but I have been
reviewing for 4+ years with this (implicitly) in mind already.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://austintripp.ca/blog/2025-06-22-ml-conference-review-guide/"&gt;Read more…&lt;/a&gt; (6 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>_all-time-best</category><category>machine learning</category><category>peer review</category><guid>https://austintripp.ca/blog/2025-06-22-ml-conference-review-guide/</guid><pubDate>Sun, 22 Jun 2025 00:00:00 GMT</pubDate></item><item><title>Punishing poor reviewers at CVPR</title><link>https://austintripp.ca/blog/2025-03-04-punish-bad-reviewers/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;p&gt;This year &lt;a href="https://cvpr.thecvf.com/"&gt;CVPR&lt;/a&gt; pledged to make all authors
participate in peer review, and reject papers from authors who wrote
low-quality reviews.&lt;sup id="fnref:ref"&gt;&lt;a class="footnote-ref" href="https://austintripp.ca/blog/2025-03-04-punish-bad-reviewers/#fn:ref"&gt;1&lt;/a&gt;&lt;/sup&gt; Last week they &lt;a href="https://x.com/CVPR/status/1894853624200863958"&gt;confirmed on
Twitter&lt;/a&gt; that they followed
through with this and rejected 19 papers. Presumably this is a tiny fraction of
the overall papers submitted, but I hope this is an effective deterrent for
future authors. At the very least, I'm glad that &lt;em&gt;some&lt;/em&gt; major conference tried
something like this!&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Update 2025-06-25&lt;/strong&gt;: NeurIPS is trying something similar now, see
&lt;a href="https://blog.neurips.cc/2025/05/02/responsible-reviewing-initiative-for-neurips-2025/"&gt;https://blog.neurips.cc/2025/05/02/responsible-reviewing-initiative-for-neurips-2025/&lt;/a&gt;.
In particular:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Reviewers who are also authors &lt;em&gt;will not be able to see their own papers'
  reviews until they submit their own reviews&lt;/em&gt;. This also extends to their
  co-authors.&lt;/li&gt;
&lt;li&gt;Once again, authors who are alos reviewers can have their own papers
  desk-rejected if they submit poor reviews.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Overall this seems like a good initiative, but I hope this doesn't cause people
to simply not review at all...&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn:ref"&gt;
&lt;p&gt;&lt;a href="https://cvpr.thecvf.com/Conferences/2025/CVPRChanges"&gt;https://cvpr.thecvf.com/Conferences/2025/CVPRChanges&lt;/a&gt; &lt;a class="footnote-backref" href="https://austintripp.ca/blog/2025-03-04-punish-bad-reviewers/#fnref:ref" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description><category>machine learning</category><category>opinion</category><category>peer review</category><guid>https://austintripp.ca/blog/2025-03-04-punish-bad-reviewers/</guid><pubDate>Tue, 04 Mar 2025 00:00:00 GMT</pubDate></item><item><title>Why don't ML conferences provide reviewer instructions?</title><link>https://austintripp.ca/blog/2025-02-25-no-reviewer-instructions/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;div&gt;&lt;p&gt;I remember when I first received an invitation to review papers for an ML
conference in late 2020. What surprised me most was &lt;em&gt;not&lt;/em&gt; that I was being
invited (even though that was a surprise, since I was just a second year PhD
student who had only just completed writing a paper myself). Instead, it was
the lack of instruction of how to assess the papers: essentially just "write
your reviews by date X", and "evaluate novelty, significance, soundness, etc".
In fact, in all the years since, I think I have &lt;em&gt;never&lt;/em&gt; received explicit
instructions for reviewing ML conference papers.&lt;sup id="fnref:instructions"&gt;&lt;a class="footnote-ref" href="https://austintripp.ca/blog/2025-02-25-no-reviewer-instructions/#fn:instructions"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://austintripp.ca/blog/2025-02-25-no-reviewer-instructions/"&gt;Read more…&lt;/a&gt; (1 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>machine learning</category><category>peer review</category><category>speculation</category><guid>https://austintripp.ca/blog/2025-02-25-no-reviewer-instructions/</guid><pubDate>Tue, 25 Feb 2025 00:00:00 GMT</pubDate></item><item><title>Stock responses about statistical significance for reviewing machine learning papers</title><link>https://austintripp.ca/blog/2025-02-11-peer-review-stat-tests/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;div&gt;&lt;p&gt;So many ML papers contain tables like&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Method&lt;/th&gt;
&lt;th&gt;Score(↑)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Baseline 1&lt;/td&gt;
&lt;td&gt;49.9%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Baseline 2&lt;/td&gt;
&lt;td&gt;49.8%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Baseline 3&lt;/td&gt;
&lt;td&gt;50.0%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Our super fancy SOTA method&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;50.1%&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;then say "results on the benchmark show that our method is state-of-the-art for task X."&lt;/p&gt;
&lt;p&gt;&lt;a href="https://austintripp.ca/blog/2025-02-11-peer-review-stat-tests/"&gt;Read more…&lt;/a&gt; (4 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>machine learning</category><category>peer review</category><guid>https://austintripp.ca/blog/2025-02-11-peer-review-stat-tests/</guid><pubDate>Tue, 11 Feb 2025 00:00:00 GMT</pubDate></item></channel></rss>