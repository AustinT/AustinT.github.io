<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Austin Tripp's website (Posts about speculation)</title><link>https://austintripp.ca/</link><description></description><atom:link href="https://austintripp.ca/categories/speculation.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents ¬© 2026 &lt;a href="mailto:austin.james.tripp[at]gmail.com"&gt;Austin Tripp&lt;/a&gt; MIT License</copyright><lastBuildDate>Sun, 01 Feb 2026 12:17:09 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Predictions for ML/AI in 2026 (and 2025 predictions re-visited).</title><link>https://austintripp.ca/blog/2026-01-05-predictions26/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;div&gt;&lt;p&gt;Last year &lt;a href="https://austintripp.ca/blog/2025-01-01-neurips24-and-trends25/"&gt;I made a bunch of predictions about
ML&lt;/a&gt;, and since 2025 is over it's time
to grade myself and repeat this exercise.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://austintripp.ca/blog/2026-01-05-predictions26/"&gt;Read more‚Ä¶&lt;/a&gt; (4 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>_recent-highlight</category><category>machine learning</category><category>speculation</category><guid>https://austintripp.ca/blog/2026-01-05-predictions26/</guid><pubDate>Mon, 05 Jan 2026 01:00:00 GMT</pubDate></item><item><title>Blogging in the LLM age: a second golden age for blogs?</title><link>https://austintripp.ca/blog/2025-06-23-blog-in-llm-age/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;div&gt;&lt;p&gt;LLMs (large language models) are currently scraping &lt;em&gt;all&lt;/em&gt; text on the public
internet, training on it, and spitting out variants of that text in response to
queries. I think this fact makes now a &lt;em&gt;golden age&lt;/em&gt; for blog writing. If you
have ever thought about writing a blog, the time is now.&lt;/p&gt;
&lt;p&gt;This idea is not unique or original&lt;sup id="fnref:gwern"&gt;&lt;a class="footnote-ref" href="https://austintripp.ca/blog/2025-06-23-blog-in-llm-age/#fn:gwern"&gt;1&lt;/a&gt;&lt;/sup&gt;, but I am completely convinced by it. The
purpose of this post is to explain it in my own words.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://austintripp.ca/blog/2025-06-23-blog-in-llm-age/"&gt;Read more‚Ä¶&lt;/a&gt; (3 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>_all-time-highlight</category><category>blog</category><category>speculation</category><guid>https://austintripp.ca/blog/2025-06-23-blog-in-llm-age/</guid><pubDate>Mon, 23 Jun 2025 00:00:00 GMT</pubDate></item><item><title>Why your active learning algorithm may not do better than random</title><link>https://austintripp.ca/blog/2025-04-02-active-learning-random/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;div&gt;&lt;p&gt;I am a big fan of active learning, but I am also acutely aware of its potential
failure modes. A common failure mode is &lt;em&gt;random-like&lt;/em&gt; performance: achieving no
better "success"&lt;sup id="fnref:success"&gt;&lt;a class="footnote-ref" href="https://austintripp.ca/blog/2025-04-02-active-learning-random/#fn:success"&gt;1&lt;/a&gt;&lt;/sup&gt; in picking points than a &lt;em&gt;random&lt;/em&gt; policy. Indeed, it
is possible to experience this when the implementation is flawed.&lt;sup id="fnref:wrong"&gt;&lt;a class="footnote-ref" href="https://austintripp.ca/blog/2025-04-02-active-learning-random/#fn:wrong"&gt;2&lt;/a&gt;&lt;/sup&gt;
However, in some problems it &lt;em&gt;may not be possible&lt;/em&gt; to beat random-like
performance. In this post I try to explain why.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://austintripp.ca/blog/2025-04-02-active-learning-random/"&gt;Read more‚Ä¶&lt;/a&gt; (3 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>machine learning</category><category>speculation</category><guid>https://austintripp.ca/blog/2025-04-02-active-learning-random/</guid><pubDate>Wed, 02 Apr 2025 00:00:00 GMT</pubDate></item><item><title>Generic recommendations for cheminformatics models</title><link>https://austintripp.ca/blog/2025-04-01-cheminformatics-model-qa/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;div&gt;&lt;p&gt;I was recently asked via email for my thoughts on which models to use &lt;em&gt;in
general&lt;/em&gt; for molecular property prediction. I'm sharing my responses publicly
in case they are useful.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://austintripp.ca/blog/2025-04-01-cheminformatics-model-qa/"&gt;Read more‚Ä¶&lt;/a&gt; (1 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>chemistry</category><category>machine learning</category><category>speculation</category><guid>https://austintripp.ca/blog/2025-04-01-cheminformatics-model-qa/</guid><pubDate>Tue, 01 Apr 2025 00:00:00 GMT</pubDate></item><item><title>Conceptual confusion about desirable outputs of reaction prediction models.</title><link>https://austintripp.ca/blog/2025-03-27-reaction-correctness/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;div&gt;&lt;p&gt;In the literature about machine learning for retrosynthesis, one line of work
tries to predict chemical reactions, either in the &lt;em&gt;forward direction&lt;/em&gt; (ie what
products will A + B form) or in the &lt;em&gt;backward direction&lt;/em&gt; (ie what reactants
could react to produce molecule C). Such models are often trained on datasets
of known reactions like Pistachio or USPTO, with the hope of generalizing to
new "correct" reactions. However, this formulation of the problem overlooks a
lot of subtleties about what makes a reaction "correct". In this post I will
present a more nuanced mental model which (hopefully) clarifies some
ambiguities.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://austintripp.ca/blog/2025-03-27-reaction-correctness/"&gt;Read more‚Ä¶&lt;/a&gt; (3 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>chemistry</category><category>machine learning</category><category>retrosynthesis</category><category>speculation</category><guid>https://austintripp.ca/blog/2025-03-27-reaction-correctness/</guid><pubDate>Thu, 27 Mar 2025 00:00:00 GMT</pubDate></item><item><title>Why don't ML conferences provide reviewer instructions?</title><link>https://austintripp.ca/blog/2025-02-25-no-reviewer-instructions/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;div&gt;&lt;p&gt;I remember when I first received an invitation to review papers for an ML
conference in late 2020. What surprised me most was &lt;em&gt;not&lt;/em&gt; that I was being
invited (even though that was a surprise, since I was just a second year PhD
student who had only just completed writing a paper myself). Instead, it was
the lack of instruction of how to assess the papers: essentially just "write
your reviews by date X", and "evaluate novelty, significance, soundness, etc".
In fact, in all the years since, I think I have &lt;em&gt;never&lt;/em&gt; received explicit
instructions for reviewing ML conference papers.&lt;sup id="fnref:instructions"&gt;&lt;a class="footnote-ref" href="https://austintripp.ca/blog/2025-02-25-no-reviewer-instructions/#fn:instructions"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://austintripp.ca/blog/2025-02-25-no-reviewer-instructions/"&gt;Read more‚Ä¶&lt;/a&gt; (1 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>machine learning</category><category>peer review</category><category>speculation</category><guid>https://austintripp.ca/blog/2025-02-25-no-reviewer-instructions/</guid><pubDate>Tue, 25 Feb 2025 00:00:00 GMT</pubDate></item><item><title>Is offline model-based optimization a realistic problem? (I'm not convinced)</title><link>https://austintripp.ca/blog/2025-02-14-is-offline-mbo-realistic/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;div&gt;&lt;div class="alert alert-info"&gt;
This is a "quickpost": a post which I have tried to write quickly, without very much editing/polishing.
For more details on quickposts, see
&lt;a href="https://austintripp.ca/blog/2025-02-11-lowering-quality/"&gt;this blog post&lt;/a&gt;.
&lt;/div&gt;

&lt;p&gt;Offline model-based optimization (OMBO in this post) is essentially 1-shot
optimization using a fixed dataset. You see data, do whatever you want, then
propose a batch of query points, which are then evaluated. Hopefully, one (or
most) of the query points are optimal (or near optimal). End of task.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://austintripp.ca/blog/2025-02-14-is-offline-mbo-realistic/"&gt;Read more‚Ä¶&lt;/a&gt; (4 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>_recent-highlight</category><category>machine learning</category><category>speculation</category><guid>https://austintripp.ca/blog/2025-02-14-is-offline-mbo-realistic/</guid><pubDate>Fri, 14 Feb 2025 00:00:00 GMT</pubDate></item><item><title>Reaction model scores are CRITICAL to multi-step retrosynthesis</title><link>https://austintripp.ca/blog/2025-01-26-reaction-model-scores/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;div&gt;&lt;p&gt;Machine-learning for retrosynthesis is a popular research topic. Popular
sub-topics include:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://austintripp.ca/blog/2025-01-26-reaction-model-scores/"&gt;Read more‚Ä¶&lt;/a&gt; (4 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>chemistry</category><category>machine learning</category><category>retrosynthesis</category><category>speculation</category><guid>https://austintripp.ca/blog/2025-01-26-reaction-model-scores/</guid><pubDate>Sun, 26 Jan 2025 00:00:00 GMT</pubDate></item><item><title>What ML researchers and users get wrong: optimistic assumptions</title><link>https://austintripp.ca/blog/2025-01-09-optimistic-ml-assumptions/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;div&gt;&lt;p&gt;ML is often done poorly, both by "ML experts" (by which I mean people who
understand the algorithms but not the data) and "ML users" (by which I mean
people who understand their data, but not the algorithms). I think the cause is
often over-optimism, although about different things:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://austintripp.ca/blog/2025-01-09-optimistic-ml-assumptions/"&gt;Read more‚Ä¶&lt;/a&gt; (1 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>speculation</category><guid>https://austintripp.ca/blog/2025-01-09-optimistic-ml-assumptions/</guid><pubDate>Thu, 09 Jan 2025 00:00:00 GMT</pubDate></item><item><title>Review of NeurIPS 2024 and predictions for ML in 2025</title><link>https://austintripp.ca/blog/2025-01-01-neurips24-and-trends25/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;div&gt;&lt;p&gt;I was fortunate to attend NeurIPS 2024, arguably the largest and most
influential machine learning conference in the world (thanks Valence for
sponsoring my trip üôè). In this post I will try to summarize what I learned at
NeurIPS, and cautiously make some predictions for the year ahead.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://austintripp.ca/blog/2025-01-01-neurips24-and-trends25/"&gt;Read more‚Ä¶&lt;/a&gt; (7 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>machine learning</category><category>speculation</category><guid>https://austintripp.ca/blog/2025-01-01-neurips24-and-trends25/</guid><pubDate>Wed, 01 Jan 2025 00:00:00 GMT</pubDate></item></channel></rss>