<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Austin Tripp's website (Posts about speculation)</title><link>https://austintripp.ca/</link><description></description><atom:link href="https://austintripp.ca/categories/speculation.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents ¬© 2025 &lt;a href="mailto:austin.james.tripp[at]gmail.com"&gt;Austin Tripp&lt;/a&gt; MIT License</copyright><lastBuildDate>Mon, 14 Apr 2025 08:35:16 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Why your active learning algorithm may not do better than random</title><link>https://austintripp.ca/blog/2025-04-02-active-learning-random/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;div&gt;&lt;p&gt;I am a big fan of active learning, but I am also acutely aware of its potential
failure modes. A common failure mode is &lt;em&gt;random-like&lt;/em&gt; performance: achieving no
better "success"&lt;sup id="fnref:success"&gt;&lt;a class="footnote-ref" href="https://austintripp.ca/blog/2025-04-02-active-learning-random/#fn:success"&gt;1&lt;/a&gt;&lt;/sup&gt; in picking points than a &lt;em&gt;random&lt;/em&gt; policy. Indeed, it
is possible to experience this when the implementation is flawed.&lt;sup id="fnref:wrong"&gt;&lt;a class="footnote-ref" href="https://austintripp.ca/blog/2025-04-02-active-learning-random/#fn:wrong"&gt;2&lt;/a&gt;&lt;/sup&gt;
However, in some problems it &lt;em&gt;may not be possible&lt;/em&gt; to beat random-like
performance. In this post I try to explain why.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://austintripp.ca/blog/2025-04-02-active-learning-random/"&gt;Read more‚Ä¶&lt;/a&gt; (3 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>machine learning</category><category>speculation</category><guid>https://austintripp.ca/blog/2025-04-02-active-learning-random/</guid><pubDate>Wed, 02 Apr 2025 00:00:00 GMT</pubDate></item><item><title>Generic recommendations for cheminformatics models.</title><link>https://austintripp.ca/blog/2025-04-01-cheminformatics-model-qa/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;div&gt;&lt;p&gt;I was recently asked via email for my thoughts on which models to use &lt;em&gt;in
general&lt;/em&gt; for molecular property prediction. I'm sharing my responses publicly
in case they are useful.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://austintripp.ca/blog/2025-04-01-cheminformatics-model-qa/"&gt;Read more‚Ä¶&lt;/a&gt; (1 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>machine learning</category><category>speculation</category><guid>https://austintripp.ca/blog/2025-04-01-cheminformatics-model-qa/</guid><pubDate>Tue, 01 Apr 2025 00:00:00 GMT</pubDate></item><item><title>Conceptual confusion about desirable outputs of reaction prediction models.</title><link>https://austintripp.ca/blog/2025-03-27-reaction-correctness/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;div&gt;&lt;p&gt;In the literature about machine learning for retrosynthesis, one line of work
tries to predict chemical reactions, either in the &lt;em&gt;forward direction&lt;/em&gt; (ie what
products will A + B form) or in the &lt;em&gt;backward direction&lt;/em&gt; (ie what reactants
could react to produce molecule C). Such models are often trained on datasets
of known reactions like Pistachio or USPTO, with the hope of generalizing to
new "correct" reactions. However, this formulation of the problem overlooks a
lot of subtleties about what makes a reaction "correct". In this post I will
present a more nuanced mental model which (hopefully) clarifies some
ambiguities.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://austintripp.ca/blog/2025-03-27-reaction-correctness/"&gt;Read more‚Ä¶&lt;/a&gt; (3 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>machine learning</category><category>retrosynthesis</category><category>speculation</category><guid>https://austintripp.ca/blog/2025-03-27-reaction-correctness/</guid><pubDate>Thu, 27 Mar 2025 00:00:00 GMT</pubDate></item><item><title>Why don't ML conferences provide reviewer instructions?</title><link>https://austintripp.ca/blog/2025-02-25-no-reviewer-instructions/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;div&gt;&lt;p&gt;I remember when I first received an invitation to review papers for an ML
conference in late 2020. What surprised me most was &lt;em&gt;not&lt;/em&gt; that I was being
invited (even though that was a surprise, since I was just a second year PhD
student who had only just completed writing a paper myself). Instead, it was
the lack of instruction of how to assess the papers: essentially just "write
your reviews by date X", and "evaluate novelty, significance, soundness, etc".
In fact, in all the years since, I think I have &lt;em&gt;never&lt;/em&gt; received explicit
instructions for reviewing ML conference papers.&lt;sup id="fnref:instructions"&gt;&lt;a class="footnote-ref" href="https://austintripp.ca/blog/2025-02-25-no-reviewer-instructions/#fn:instructions"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://austintripp.ca/blog/2025-02-25-no-reviewer-instructions/"&gt;Read more‚Ä¶&lt;/a&gt; (1 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>machine learning</category><category>peer review</category><category>speculation</category><guid>https://austintripp.ca/blog/2025-02-25-no-reviewer-instructions/</guid><pubDate>Tue, 25 Feb 2025 00:00:00 GMT</pubDate></item><item><title>Is offline model-based optimization a realistic problem? (I'm not convinced)</title><link>https://austintripp.ca/blog/2025-02-14-is-offline-mbo-realistic/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;div&gt;&lt;div class="alert alert-info"&gt;
This is a "quickpost": a post which I have tried to write quickly, without very much editing/polishing.
For more details on quickposts, see
&lt;a href="https://austintripp.ca/blog/2025-02-11-lowering-quality/"&gt;this blog post&lt;/a&gt;.
&lt;/div&gt;

&lt;p&gt;Offline model-based optimization (OMBO in this post) is essentially 1-shot
optimization using a fixed dataset. You see data, do whatever you want, then
propose a batch of query points, which are then evaluated. Hopefully, one (or
most) of the query points are optimal (or near optimal). End of task.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://austintripp.ca/blog/2025-02-14-is-offline-mbo-realistic/"&gt;Read more‚Ä¶&lt;/a&gt; (4 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>machine learning</category><category>speculation</category><guid>https://austintripp.ca/blog/2025-02-14-is-offline-mbo-realistic/</guid><pubDate>Fri, 14 Feb 2025 00:00:00 GMT</pubDate></item><item><title>Reaction model scores are CRITICAL to multi-step retrosynthesis.</title><link>https://austintripp.ca/blog/2025-01-26-reaction-model-scores/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;div&gt;&lt;p&gt;Machine-learning for retrosynthesis is a popular research topic. Popular
sub-topics include:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://austintripp.ca/blog/2025-01-26-reaction-model-scores/"&gt;Read more‚Ä¶&lt;/a&gt; (4 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>machine learning</category><category>retrosynthesis</category><category>speculation</category><guid>https://austintripp.ca/blog/2025-01-26-reaction-model-scores/</guid><pubDate>Sun, 26 Jan 2025 00:00:00 GMT</pubDate></item><item><title>What ML researchers and users get wrong: optimistic assumptions</title><link>https://austintripp.ca/blog/2025-01-09-optimistic-ml-assumptions/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;div&gt;&lt;p&gt;ML is often done poorly, both by "ML experts" (by which I mean people who
understand the algorithms but not the data) and "ML users" (by which I mean
people who understand their data, but not the algorithms). I think the cause is
often over-optimism, although about different things:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://austintripp.ca/blog/2025-01-09-optimistic-ml-assumptions/"&gt;Read more‚Ä¶&lt;/a&gt; (1 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>speculation</category><guid>https://austintripp.ca/blog/2025-01-09-optimistic-ml-assumptions/</guid><pubDate>Thu, 09 Jan 2025 00:00:00 GMT</pubDate></item><item><title>Review of NeurIPS 2024 and predictions for ML in 2025</title><link>https://austintripp.ca/blog/2025-01-01-neurips24-and-trends25/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;div&gt;&lt;p&gt;I was fortunate to attend NeurIPS 2024, arguably the largest and most
influential machine learning conference in the world (thanks Valence for
sponsoring my trip üôè). In this post I will try to summarize what I learned at
NeurIPS, and cautiously make some predictions for the year ahead.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://austintripp.ca/blog/2025-01-01-neurips24-and-trends25/"&gt;Read more‚Ä¶&lt;/a&gt; (7 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>machine learning</category><category>speculation</category><guid>https://austintripp.ca/blog/2025-01-01-neurips24-and-trends25/</guid><pubDate>Wed, 01 Jan 2025 00:00:00 GMT</pubDate></item><item><title>Rules of scientific English writing for an international audience.</title><link>https://austintripp.ca/blog/2024-12-30-international-english/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;div&gt;&lt;p&gt;Although English is the common language for international scientific
communication, most scientists are not native English speakers. To account for
this, I think that all scientists (especially native English speakers) should
try to write text which is easy to read for non-native speakers. I propose the
following rules for this:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://austintripp.ca/blog/2024-12-30-international-english/"&gt;Read more‚Ä¶&lt;/a&gt; (11 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>language</category><category>speculation</category><category>writing</category><guid>https://austintripp.ca/blog/2024-12-30-international-english/</guid><pubDate>Mon, 30 Dec 2024 00:00:00 GMT</pubDate></item><item><title>When should you expect Bayesian optimization to work well?</title><link>https://austintripp.ca/blog/2024-12-10-when-should-bo-work/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;div&gt;&lt;p&gt;As much as I believe in the potential of Bayesian optimization (BO) to be
useful for scientific discovery, after 4+ years I have seen many instances
where BO &lt;em&gt;does not&lt;/em&gt; work. In this post I explain a simple heuristic rule to
decide whether you should expect BO to work well or not.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://austintripp.ca/blog/2024-12-10-when-should-bo-work/"&gt;Read more‚Ä¶&lt;/a&gt; (3 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>bayesian optimization</category><category>machine learning</category><category>speculation</category><guid>https://austintripp.ca/blog/2024-12-10-when-should-bo-work/</guid><pubDate>Tue, 10 Dec 2024 00:00:00 GMT</pubDate></item></channel></rss>