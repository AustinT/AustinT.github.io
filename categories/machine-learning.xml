<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Austin Tripp's website (Posts about machine learning)</title><link>https://austintripp.ca/</link><description></description><atom:link href="https://austintripp.ca/categories/machine-learning.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2024 &lt;a href="mailto:austin.james.tripp[at]gmail.com"&gt;Austin Tripp&lt;/a&gt; MIT License</copyright><lastBuildDate>Fri, 13 Sep 2024 16:57:00 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Sparse Matrices: 5 Tips and Tricks</title><link>https://austintripp.ca/blog/2018/09/12/sparse-matrices-tips1/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;div&gt;&lt;p&gt;Over the course of my internship at the online shopping company &lt;a href="https://www.wish.com/"&gt;Wish&lt;/a&gt;, I have dealt a lot with a lot of data in the form of sparse matrices, specificaly in the form of item interaction matrices for customer data. In doing so, I have made heavy use of &lt;a href="https://docs.scipy.org/doc/scipy/reference/sparse.html"&gt;scipy's sparse matrices library&lt;/a&gt;.
Here are 5 tricks that I have learned.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://austintripp.ca/blog/2018/09/12/sparse-matrices-tips1/"&gt;Read more…&lt;/a&gt; (2 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>frontpage</category><category>machine learning</category><guid>https://austintripp.ca/blog/2018/09/12/sparse-matrices-tips1/</guid><pubDate>Tue, 11 Sep 2018 23:00:00 GMT</pubDate></item><item><title>An Overview of Gradient Boosting and Popular Libraries for it.</title><link>https://austintripp.ca/blog/2018/07/17/gradient-boosting/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;div&gt;&lt;p&gt;Everybody doing machine learning wants the best models possible. 
The aim of this blog article is the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;To provide an introduction to the machine learning technique known as &lt;em&gt;boosting&lt;/em&gt;, and specifically &lt;em&gt;gradient boosting&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;To compare/contrast boosting with other ensemble methods, such as &lt;em&gt;bagging&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;To explain and compare several popular gradient boosting frameworks, specifically &lt;em&gt;XGBoost&lt;/em&gt;, &lt;em&gt;CatBoost&lt;/em&gt;, and &lt;em&gt;LightGBM&lt;/em&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a href="https://austintripp.ca/blog/2018/07/17/gradient-boosting/"&gt;Read more…&lt;/a&gt; (7 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>frontpage</category><category>machine learning</category><guid>https://austintripp.ca/blog/2018/07/17/gradient-boosting/</guid><pubDate>Mon, 16 Jul 2018 23:00:00 GMT</pubDate></item><item><title>Turning Adam Optimization into SGD</title><link>https://austintripp.ca/blog/2018/07/02/adam-to-sgd/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;div&gt;&lt;h3 id="motivation"&gt;Motivation&lt;/h3&gt;
&lt;p&gt;This strange question came up when working on a machine learning project to generate embeddings. 
Working with the version of Pytorch available on our DGX (similar to version 0.3.1), I found there was an optimizer called &lt;em&gt;SparseAdam&lt;/em&gt; but not one called &lt;em&gt;SparseSGD&lt;/em&gt;.
Since what I really wanted to do was use SGD, I wondered: could I turn the Adam optimizer into an SGD optimizer by setting the hyperparameters \(\beta_1\), \(\beta_2\), and \(\epsilon\)?&lt;/p&gt;
&lt;p&gt;&lt;a href="https://austintripp.ca/blog/2018/07/02/adam-to-sgd/"&gt;Read more…&lt;/a&gt; (1 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>frontpage</category><category>machine learning</category><guid>https://austintripp.ca/blog/2018/07/02/adam-to-sgd/</guid><pubDate>Sun, 01 Jul 2018 23:00:00 GMT</pubDate></item></channel></rss>