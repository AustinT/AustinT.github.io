<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Austin Tripp's website (Posts about machine learning)</title><link>https://austintripp.ca/</link><description></description><atom:link href="https://austintripp.ca/categories/machine-learning.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2026 &lt;a href="mailto:austin.james.tripp[at]gmail.com"&gt;Austin Tripp&lt;/a&gt; MIT License</copyright><lastBuildDate>Sun, 04 Jan 2026 14:41:44 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Review of NeurIPS 2025</title><link>https://austintripp.ca/blog/2025-12-18-neurips25/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;div&gt;&lt;p&gt;I added the NeurIPS &lt;em&gt;workshops&lt;/em&gt; this year (not the main conference).&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="https://austintripp.ca/blog/2025-12-18-neurips25/#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt; Here
are my takeaways. I'll follow up with predictions for 2026 in a later post.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://austintripp.ca/blog/2025-12-18-neurips25/"&gt;Read more…&lt;/a&gt; (1 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>_recent-highlight</category><category>machine learning</category><guid>https://austintripp.ca/blog/2025-12-18-neurips25/</guid><pubDate>Thu, 18 Dec 2025 00:00:00 GMT</pubDate></item><item><title>Finally, an ML conference review guide!</title><link>https://austintripp.ca/blog/2025-11-04-finally-a-reviewer-guide/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;div&gt;&lt;p&gt;ICLR 2026 released a detailed review guide as part of its review process
(&lt;a href="https://iclr.cc/Conferences/2026/ReviewerGuide"&gt;link&lt;/a&gt;). Let's analyze it!&lt;/p&gt;
&lt;p&gt;&lt;a href="https://austintripp.ca/blog/2025-11-04-finally-a-reviewer-guide/"&gt;Read more…&lt;/a&gt; (3 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>machine learning</category><category>peer review</category><guid>https://austintripp.ca/blog/2025-11-04-finally-a-reviewer-guide/</guid><pubDate>Tue, 04 Nov 2025 00:00:00 GMT</pubDate></item><item><title>Justifying expected utility maximization from first principles (Von Neumann–Morgenstern).</title><link>https://austintripp.ca/blog/2025-11-02-expected-utility/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;div&gt;&lt;p&gt;I've recently found myself arguing for expected utility maximization as an
approach to practical decision making problems at work (mostly regarding
molecule selection). This post is my attempt to write out an argument for using
expected utility, targeted at non-mathematical (but still technical) readers.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://austintripp.ca/blog/2025-11-02-expected-utility/"&gt;Read more…&lt;/a&gt; (7 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>_recent-highlight</category><category>machine learning</category><category>opinion</category><category>statistics</category><guid>https://austintripp.ca/blog/2025-11-02-expected-utility/</guid><pubDate>Sun, 02 Nov 2025 00:00:00 GMT</pubDate></item><item><title>Latent Space COWBOYS: a VAE-BO method I can actually buy into!</title><link>https://austintripp.ca/blog/2025-10-05-latent-space-cowboys/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;div&gt;&lt;p&gt;I am generally pessimistic about BO (Bayesian optimization) methods which use
VAE embeddings as part of the model. Mostly this is because distance in a VAE's
latent space has no reason to correlate with distances in property space
(unless trained for it), and because training with labels is basically deep
kernel learning which usually overfits&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="https://austintripp.ca/blog/2025-10-05-latent-space-cowboys/#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt;. However, I recently came across the
paper "&lt;em&gt;Return of the Latent Space COWBOYS: Re-thinking the use of VAEs for
Bayesian Optimisation of Structured Spaces&lt;/em&gt;"&lt;sup id="fnref:2"&gt;&lt;a class="footnote-ref" href="https://austintripp.ca/blog/2025-10-05-latent-space-cowboys/#fn:2"&gt;2&lt;/a&gt;&lt;/sup&gt; and quite liked the idea,
ending my long-standing streak of disliking every VAE-BO paper I read.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://austintripp.ca/blog/2025-10-05-latent-space-cowboys/"&gt;Read more…&lt;/a&gt; (1 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>bayesian optimization</category><category>machine learning</category><category>opinion</category><category>research papers</category><guid>https://austintripp.ca/blog/2025-10-05-latent-space-cowboys/</guid><pubDate>Sun, 05 Oct 2025 00:00:00 GMT</pubDate></item><item><title>An accessible introduction to mutual information using a d20</title><link>https://austintripp.ca/blog/2025-08-10-mutual-information-introduction/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered" id="cell-id=8c7d68eb-f957-4e21-8632-74b01147a07c"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;Scientists want to do "informative" experiments. What makes an experiment informative? I believe that information theory has a lot of answers. In this post I will try to explain information theory's definition of "how much information you get from an experiment" using d20 dice, assuming no previous knowledge of information theory.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://austintripp.ca/blog/2025-08-10-mutual-information-introduction/"&gt;Read more…&lt;/a&gt; (16 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>_recent-highlight</category><category>machine learning</category><category>statistics</category><guid>https://austintripp.ca/blog/2025-08-10-mutual-information-introduction/</guid><pubDate>Sun, 10 Aug 2025 00:00:00 GMT</pubDate></item><item><title>My review guide for machine learning conference papers</title><link>https://austintripp.ca/blog/2025-06-22-ml-conference-review-guide/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;div&gt;&lt;p&gt;There is no official step by step guide for how to review ML conference papers
at venues like NeurIPS/ICML/ICLR.&lt;sup id="fnref:prev"&gt;&lt;a class="footnote-ref" href="https://austintripp.ca/blog/2025-06-22-ml-conference-review-guide/#fn:prev"&gt;1&lt;/a&gt;&lt;/sup&gt; In this post, I try to explain &lt;em&gt;my
guide&lt;/em&gt;. It is not official, endorsed, or necessarily good, but I have been
reviewing for 4+ years with this (implicitly) in mind already.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://austintripp.ca/blog/2025-06-22-ml-conference-review-guide/"&gt;Read more…&lt;/a&gt; (6 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>_all-time-best</category><category>machine learning</category><category>peer review</category><guid>https://austintripp.ca/blog/2025-06-22-ml-conference-review-guide/</guid><pubDate>Sun, 22 Jun 2025 00:00:00 GMT</pubDate></item><item><title>Problems with ML for toxicity prediction.</title><link>https://austintripp.ca/blog/2025-05-13-ml-toxicity-pred/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;div&gt;&lt;p&gt;I recently read &lt;a href="https://pubs.acs.org/doi/10.1021/acs.chemrestox.5c00033"&gt;a review paper by Seal et
al&lt;/a&gt; about machine
learning for toxicity prediction. Given the length of the paper I thought I
would share my important takeaways. Disclaimer: not everything in this post was
part of the paper, and not everything in the paper is reflected in this post.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://austintripp.ca/blog/2025-05-13-ml-toxicity-pred/"&gt;Read more…&lt;/a&gt; (4 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>chemistry</category><category>machine learning</category><category>medicine</category><category>research papers</category><guid>https://austintripp.ca/blog/2025-05-13-ml-toxicity-pred/</guid><pubDate>Tue, 13 May 2025 00:00:00 GMT</pubDate></item><item><title>Chebyshev Scalarization Explained</title><link>https://austintripp.ca/blog/2025-05-12-chebyshev-scalarization/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;div&gt;&lt;p&gt;I've been reading about multi-objective optimization recently.&lt;sup id="fnref:surprise"&gt;&lt;a class="footnote-ref" href="https://austintripp.ca/blog/2025-05-12-chebyshev-scalarization/#fn:surprise"&gt;1&lt;/a&gt;&lt;/sup&gt;
Many papers state limitations of "linear scalarization" approaches,
mainly that it might not be able to represent all Pareto-optimal solutions
(if this sentence does not make sense to you, see &lt;a href="https://austintripp.ca/blog/2025-05-12-chebyshev-scalarization/#background-to-multi-objective-optimization"&gt;background&lt;/a&gt;).
Chebyshev scalarization is sometimes mentioned as an alternative which &lt;em&gt;can&lt;/em&gt; represent all solutions.
However, these papers mention it in passing without a proper explanation,
and I did not find a good explanation of it online.&lt;sup id="fnref:online"&gt;&lt;a class="footnote-ref" href="https://austintripp.ca/blog/2025-05-12-chebyshev-scalarization/#fn:online"&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;After doing a bit of my own research,&lt;sup id="fnref:gemini"&gt;&lt;a class="footnote-ref" href="https://austintripp.ca/blog/2025-05-12-chebyshev-scalarization/#fn:gemini"&gt;3&lt;/a&gt;&lt;/sup&gt; I found that Chebyshev scalarization is actually not too complicated, so I thought &lt;em&gt;I&lt;/em&gt; would explain it online.
In this post, I:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Give definitions for Chebyshev scalarization (for both maximization and minimization)&lt;/li&gt;
&lt;li&gt;Give a simple proof that it can represent all Pareto-optimal solutions&lt;/li&gt;
&lt;li&gt;Explain its relationship to linear scalarization via $\ell_p$ norms.&lt;/li&gt;
&lt;li&gt;Give a geometric interpretation via an interactive visualization&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="https://austintripp.ca/blog/2025-05-12-chebyshev-scalarization/"&gt;Read more…&lt;/a&gt; (13 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>_all-time-best</category><category>bayesian optimization</category><category>machine learning</category><guid>https://austintripp.ca/blog/2025-05-12-chebyshev-scalarization/</guid><pubDate>Mon, 12 May 2025 00:00:00 GMT</pubDate></item><item><title>Taking the V out of VAEs: long live KL-regularized autoencoders!</title><link>https://austintripp.ca/blog/2025-04-24-v-out-of-vae/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;div&gt;&lt;p&gt;I recently read &lt;a href="https://sander.ai/2025/04/15/latents.html#"&gt;this post about generative modelling in latent
space&lt;/a&gt; by Sander Dieleman and
agreed strongly with the following quote (copied verbatim except for typo
correction):&lt;/p&gt;
&lt;p&gt;&lt;a href="https://austintripp.ca/blog/2025-04-24-v-out-of-vae/"&gt;Read more…&lt;/a&gt; (1 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>machine learning</category><guid>https://austintripp.ca/blog/2025-04-24-v-out-of-vae/</guid><pubDate>Thu, 24 Apr 2025 00:00:00 GMT</pubDate></item><item><title>Coding python packages with AI</title><link>https://austintripp.ca/blog/2025-04-12-ai-coding-python-package/</link><dc:creator>Austin Tripp</dc:creator><description>&lt;div&gt;&lt;p&gt;I tried using some new LLM tools to code 2 entire python packages (instead of
editing a handful of lines at a time, which is what I did previously). It went
well! These tools are not perfect, but they are useful!&lt;/p&gt;
&lt;p&gt;&lt;a href="https://austintripp.ca/blog/2025-04-12-ai-coding-python-package/"&gt;Read more…&lt;/a&gt; (3 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>machine learning</category><category>using large language models</category><guid>https://austintripp.ca/blog/2025-04-12-ai-coding-python-package/</guid><pubDate>Sat, 12 Apr 2025 00:00:00 GMT</pubDate></item></channel></rss>