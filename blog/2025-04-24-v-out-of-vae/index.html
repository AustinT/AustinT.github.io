<!DOCTYPE html>
<html prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article# " lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Taking the V out of VAEs: long live KL-regularized autoencoders! | Austin Tripp's website</title>
<link href="../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<link href="https://fonts.googleapis.com/css?family=Playfair+Display:700,900" rel="stylesheet">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="alternate" type="application/rss+xml" title="RSS" hreflang="en" href="../../rss.xml">
<link rel="canonical" href="https://austintripp.ca/blog/2025-04-24-v-out-of-vae/">
<!--[if lt IE 9]><script src="../../assets/js/html5.js"></script><![endif]--><meta name="author" content="Austin Tripp">
<link rel="prev" href="../2025-04-12-ai-coding-python-package/" title="Coding python packages with AI" type="text/html">
<link rel="next" href="../2025-04-28-beijing-travel/" title="Some Beijing travel tips" type="text/html">
<meta property="og:site_name" content="Austin Tripp's website">
<meta property="og:title" content="Taking the V out of VAEs: long live KL-regularized autoencoders!">
<meta property="og:url" content="https://austintripp.ca/blog/2025-04-24-v-out-of-vae/">
<meta property="og:description" content="I recently read this post about generative modelling in latent
space by Sander Dieleman and
agreed strongly with the following quote (copied verbatim except for typo
correction):



KL regularisation,">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2025-04-24T00:00:00Z">
<meta property="article:tag" content="machine learning">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Header and menu bar -->
<div class="container">
      <header class="blog-header py-3"><div class="row nbb-header align-items-center">
          <div class="col-md-3 col-xs-2 col-sm-2" style="width: auto;">
            <button class="navbar-toggler navbar-light bg-light nbb-navbar-toggler" type="button" data-toggle="collapse" data-target=".bs-nav-collapsible" aria-controls="bs-navbar" aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse bs-nav-collapsible bootblog4-search-form-holder">
                
            </div>
        </div>
          <div class="col-md-6 col-xs-10 col-sm-10 bootblog4-brand" style="width: auto;">
            <a class="navbar-brand blog-header-logo text-dark" href="../../">

            <span id="blog-title">Austin Tripp's website</span>
        </a>
          </div>
            <div class="col-md-3 justify-content-end align-items-center bs-nav-collapsible collapse flex-collapse bootblog4-right-nav">
            <nav class="navbar navbar-light bg-white"><ul class="navbar-nav bootblog4-right-nav">
<li class="nav-item">
    <a href="index.md" id="sourcelink" class="nav-link">Source</a>
    </li>


                    
            </ul></nav>
</div>
    </div>
</header><nav class="navbar navbar-expand-md navbar-light bg-white static-top"><div class="collapse navbar-collapse bs-nav-collapsible" id="bs-navbar">
            <ul class="navbar-nav nav-fill d-flex w-100">
<li class="nav-item">
<a href="../../" class="nav-link">Home</a>
                </li>
<li class="nav-item">
<a href="../../cv/" class="nav-link">CV</a>
                </li>
<li class="nav-item">
<a href="../../research/" class="nav-link">Research</a>
                </li>
<li class="nav-item">
<a href="../../resources/" class="nav-link">Resources</a>
                </li>
<li class="nav-item">
<a href="../" class="nav-link">Blog</a>

                
            </li>
</ul>
</div>
<!-- /.navbar-collapse -->
</nav>
</div>

<div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        
        
        
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="." class="u-url">Taking the V out of VAEs: long live KL-regularized autoencoders!</a></h1>

        <div class="metadata">
            <p class="byline author vcard p-author h-card"><span class="byline-name fn p-name" itemprop="author">
                    Austin Tripp
            </span></p>
            <p class="dateline">
            <a href="." rel="bookmark">
            <time class="published dt-published" datetime="2025-04-24T00:00:00Z" itemprop="datePublished" title="2025-04-24">2025-04-24</time></a>
            </p>
            
        <p class="sourceline"><a href="index.md" class="sourcelink">Source</a></p>

        </div>
        

    </header><div class="e-content entry-content" itemprop="articleBody text">
    <p>I recently read <a href="https://sander.ai/2025/04/15/latents.html#">this post about generative modelling in latent
space</a> by Sander Dieleman and
agreed strongly with the following quote (copied verbatim except for typo
correction):</p>
<!-- TEASER_END -->

<blockquote>
<p>KL regularisation, on the other hand, is a core part of the traditional VAE
setup: it is one of the two terms constituting the evidence lower bound
(ELBO), which bounds the likelihood from below and enables VAE training to
tractably (but indirectly) maximise the likelihood of the data. It encourages
the latents to follow the imposed prior distribution (usually Gaussian).
Crucially however, the ELBO is only truly a lower bound on the likelihood if
there is no scaling hyperparameter in front of this term. Yet almost
invariably, the KL term used to regularise continuous latent spaces is scaled
down significantly (usually by several orders of magnitude), all but severing
the connection with the variational inference context in which it originally
arose.</p>
<p>The reason is simple: an unscaled KL term has too strong an effect, imposing
a stringent limit on latent capacity and thus severely degrading
reconstruction quality. The pragmatic response to that is naturally to scale
down its relative contribution to the training loss. (Aside: in settings
where one cares less about reconstruction quality, and more about semantic
interpretability and disentanglement of the learnt
representation, increasing the scale of the KL term can also be fruitful, as
in beta-VAE.)</p>
<p>We are veering firmly into opinion territory here, but I feel there is
currently quite a bit of magical thinking around the effect of the KL term.
It is often suggested that this term encourages the latents to follow a
Gaussian distribution, but with the scale factors that are typically used,
this effect is way too weak to be meaningful. Even for “proper” VAEs, the
aggregate posterior is rarely actually Gaussian.</p>
<p>All of this renders the “V” in “VAE” basically meaningless, in my opinion –
its relevance is largely historical. We may as well drop it and talk about
KL-regularised autoencoders instead, which more accurately reflects modern
practice. The most important effect of the KL term in this setting is to
suppress outliers and constrain the scale of the latents to some extent. In
other words: while the KL term is often presented as constraining capacity,
the way it is used in practice mainly constrains the shape of the
latents (but even that effect is relatively modest).</p>
</blockquote>
<p>KL-regularized autoencoders are just one part of a whole family of regularized
autoencoders, some with a probabilistic interpretation (eg Wasserstein
autoencoders), others less so.</p>
<p>I will try to use the term "X-regularized autoencoder" myself in the future.</p>
    </div>
    <aside class="postpromonav"><nav><ul itemprop="keywords" class="tags">
<li><a class="tag p-category" href="../../categories/machine-learning/" rel="tag">machine learning</a></li>
        </ul>
<ul class="pager hidden-print">
<li class="previous">
                <a href="../2025-04-12-ai-coding-python-package/" rel="prev" title="Coding python packages with AI">Previous post</a>
            </li>
            <li class="next">
                <a href="../2025-04-28-beijing-travel/" rel="next" title="Some Beijing travel tips">Next post</a>
            </li>
        </ul></nav></aside></article><!--End of body content--><footer id="footer"><div style="text-align: center;">
<a href="mailto:austin.james.tripp[at]gmail.com">Email</a> | 
<a href="https://github.com/AustinT">GitHub</a> | 
<a href="https://scholar.google.com/citations?user=WAvRaxMAAAAJ">Scholar</a> | 
<a href="https://bsky.app/profile/austinjtripp.bsky.social">Bluesky</a> | 
<a href="https://twitter.com/austinjtripp">Twitter/X</a> 
</div>
<br>
Website powered by <a href="https://getnikola.com" rel="nofollow">Nikola</a>.
Last updated 2025-05-12.
Contents © 2025         Austin Tripp (MIT License). <br>
All opinions and statements on this site are my own (ie not my employer's).
<br></footer>
</div>
</div>


        <script src="../../assets/js/all-nocdn.js"></script><script src="../../assets/js/luxon.min.js"></script><!-- fancy dates --><script>
        luxon.Settings.defaultLocale = "en";
        fancydates(2, {"preset": false, "format": "yyyy-MM-dd HH:mm"});
        </script><!-- end fancy dates --><script>
    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element){var i=element.getElementsByTagName('img')[0];return i===undefined?'':i.alt;}});
    </script>
</body>
</html>
